{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sklearn官网：http://scikit-learn.org/stable/index.html\n",
    "- sklearn API：http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing\n",
    "\n",
    "# 获取数据\n",
    "\n",
    "机器学习算法往往需要大量的数据，在 skleran 中获取数据通常采用两种方式，一种是使用自带的数据集，另一种是创建数据集\n",
    "\n",
    "## 导入数据集\n",
    "\n",
    "sklearn 自带了很多数据集，可以用来对算法进行测试分析，免去了自己再去找数据集的烦恼\n",
    "\n",
    "其中包括：\n",
    "\n",
    "- 鸢尾花数据集: `load_iris()`\n",
    "- 手写数字数据集: `load_digitals()`\n",
    "- 糖尿病数据集: `load_diabetes()`\n",
    "- 乳腺癌数据集: `load_breast_cancer()`\n",
    "- 波士顿房价数据集: `load_boston()`\n",
    "- 体能训练数据集: `load_linnerud()`\n",
    "    \n",
    "    \n",
    "这里以鸢尾花数据集为例导入数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T14:12:26.297447Z",
     "start_time": "2018-09-02T14:12:25.618823Z"
    }
   },
   "outputs": [],
   "source": [
    "# 导入 sklearn 的数据集\n",
    "\n",
    "import sklearn.datasets as sk_datasets \n",
    "iris = sk_datasets.load_iris() \n",
    "iris_X = iris.data  # 导入数据 \n",
    "iris_y = iris.target # 导入标签 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T14:12:54.236108Z",
     "start_time": "2018-09-02T14:12:54.231148Z"
    }
   },
   "source": [
    "## 创建数据集\n",
    "\n",
    "使用 skleran 的样本生成器 (samples generator) 可以创建数据，`sklearn.datasets.samples_generator` 中包含了大量创建样本数据的方法。\n",
    "\n",
    "这里以分类问题创建样本数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T14:13:46.664717Z",
     "start_time": "2018-09-02T14:13:46.183005Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [ 0.64459602  0.92767918 -1.32091378 -1.25725859 -0.74386837]\n",
      "0: [ 1.66098845  2.22206181 -2.86249859 -3.28323172 -1.62389676]\n",
      "0: [ 0.27019475 -0.12572907  1.1003977  -0.6600737   0.58334745]\n",
      "1: [-0.77182836 -1.03692724  1.34422289  1.52452016  0.76221055]\n",
      "1: [-0.1407289   0.32675611 -1.41296696  0.4113583  -0.75833145]\n",
      "1: [-0.76656634 -0.35589955 -0.83132182  1.68841011 -0.4153836 ]\n"
     ]
    }
   ],
   "source": [
    "import sklearn.datasets.samples_generator as sk_sample_generator\n",
    "\n",
    "X, y = sk_sample_generator.make_classification(\n",
    "    n_samples=6, n_features=5, n_informative=2, n_redundant=3, n_classes=2, n_clusters_per_class=2, scale=1, random_state=20)\n",
    "\n",
    "for x_, y_ in zip(X, y):\n",
    "    print(y_, end=\": \") \n",
    "    print(x_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数说明：\n",
    "\n",
    "- `n_features` : 特征个数 = `n_informative() + n_redundant + n_repeated`\n",
    "- `n_informative`：多信息特征的个数\n",
    "- `n_redundant`：冗余信息，informative 特征的随机线性组合\n",
    "- `n_repeated` ：重复信息，随机提取 `n_informative` 和 `n_redundant` 特征\n",
    "- `n_classes`：分类类别\n",
    "- `n_clusters_per_class` ：某一个类别是由几个 cluster 构成的\n",
    "- `random_state`：随机种子，使得实验可重复\n",
    "\n",
    "`n_classes` $\\times$ `n_clusters_per_class` $\\leq$ $2^{\\text{n_informative}}$\n",
    "\n",
    "# 数据集的划分\n",
    "\n",
    "机器学习的过程正往往需要对数据集进行划分，常分为训练集，测试集。sklearn 中的 `model_selection` 为我们提供了划分数据集的方法。\n",
    "\n",
    "以鸢尾花数据集为例进行划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T14:21:57.808728Z",
     "start_time": "2018-09-02T14:21:57.803272Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import sklearn.model_selection as sk_model_selection\n",
    "X_train, X_test, y_train, y_test = sk_model_selection.train_test_split(\n",
    "    iris_X, iris_y, train_size=0.2, random_state=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数说明：\n",
    "\n",
    "- `arrays`：样本数组，包含特征向量和标签\n",
    "- `test_size`：\n",
    "　　- `float`:获得多大比重的测试样本 （默认：0.25）\n",
    "　　- `int`: 获得多少个测试样本\n",
    "- `train_size`: 同 `test_size`\n",
    "- `random_state`:`int` - 随机种子（种子固定，实验可复现）\n",
    "- `shuffle` - 是否在分割之前对数据进行洗牌（默认True）\n",
    "\n",
    "sklearn 数据集划分方法还有如下方法: KFold，GroupKFold，StratifiedKFold，LeaveOneGroupOut，LeavePGroupsOut，LeaveOneOut，LeavePOut，ShuffleSplit，GroupShuffleSplit，StratifiedShuffleSplit，PredefinedSplit，TimeSeriesSplit，\n",
    "\n",
    "## 数据集划分方法——K折交叉验证：KFold，GroupKFold，StratifiedKFold\n",
    "\n",
    "- 将全部训练集 $S$ 分成 $k$ 个不相交的子集，假设 $S$ 中的训练样例个数为 $m$，那么每一个自己有 $m/k$ 个训练样例，相应的子集为 $\\{s_1，s_2，\\cdots s_k\\}$\n",
    "- 每次从分好的子集里面，拿出一个作为测试集，其他 $k-1$ 个作为训练集\n",
    "- 在 $k-1$ 个训练集上训练出学习器模型\n",
    "- 把这个模型放到测试集上，得到分类率的平均值，作为该模型或者假设函数的真实分类率\n",
    "\n",
    "这个方法充分利用了所以样本，但计算比较繁琐，需要训练 $k$ 次，测试 $k$ 次\n",
    "\n",
    "### KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T14:40:57.701167Z",
     "start_time": "2018-09-02T14:40:57.693726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=2, random_state=None, shuffle=False)\n",
      "Train Index: [3 4 5] ,Test Index: [0 1 2]\n",
      "Train Index: [0 1 2] ,Test Index: [3 4 5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# KFold\n",
    "from sklearn.model_selection import KFold\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]])\n",
    "y = np.array([1, 2, 3, 4, 5, 6])\n",
    "kf = KFold(n_splits=2)  # 分成几个组\n",
    "kf.get_n_splits(X)\n",
    "print(kf)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"Train Index:\", train_index, \",Test Index:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    # print(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T14:41:04.552680Z",
     "start_time": "2018-09-02T14:41:04.541257Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroupKFold(n_splits=2)\n",
      "Train Index: [0 2 4] ,Test Index: [1 3 5]\n",
      "Train Index: [1 3 5] ,Test Index: [0 2 4]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]])\n",
    "y = np.array([1, 2, 3, 4, 5, 6])\n",
    "groups = np.array([1, 2, 3, 4, 5, 6])\n",
    "group_kfold = GroupKFold(n_splits=2)\n",
    "group_kfold.get_n_splits(X, y, groups)\n",
    "print(group_kfold)\n",
    "for train_index, test_index in group_kfold.split(X, y, groups):\n",
    "    print(\"Train Index:\", train_index, \",Test Index:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    # print(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StratifiedKFold\n",
    "\n",
    "保证训练集中每一类的比例是相同的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T14:41:40.673494Z",
     "start_time": "2018-09-02T14:41:40.656135Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=3, random_state=None, shuffle=False)\n",
      "Train Index: [1 2 4 5] ,Test Index: [0 3]\n",
      "Train Index: [0 2 3 5] ,Test Index: [1 4]\n",
      "Train Index: [0 1 3 4] ,Test Index: [2 5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "X=np.array([[1,2],[3,4],[5,6],[7,8],[9,10],[11,12]])\n",
    "y=np.array([1,1,1,2,2,2])\n",
    "skf=StratifiedKFold(n_splits=3)\n",
    "skf.get_n_splits(X,y)\n",
    "print(skf)\n",
    "for train_index,test_index in skf.split(X,y):\n",
    "    print(\"Train Index:\",train_index,\",Test Index:\",test_index)\n",
    "    X_train,X_test=X[train_index],X[test_index]\n",
    "    y_train,y_test=y[train_index],y[test_index]\n",
    "    #print(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集划分方法——留一法: LeaveOneGroupOut，LeavePGroupsOut，LeaveOneOut，LeavePOut\n",
    "\n",
    "留一法验证（Leave-one-out，LOO）：假设有 $N$ 个样本，将每一个样本作为测试样本，其他 $N-1$ 个样本作为训练样本，这样得到 $N$ 个分类器，$N$ 个测试结果，用这 $N$ 个结果的平均值来衡量模型的性能\n",
    "\n",
    "如果 LOO 与 K-fold CV 比较，LOO 在 $N$ 个样本上建立 $N$ 个模型而不是 $k$ 个，更进一步，$N$ 个模型的每一个都是在 $N-1$ 个样本上训练的，而不是 $(k-1)n/k$。两种方法中，假定 $k$ 不是很大而且 $k<<N$，LOO 比 k-fold CV 更耗时\n",
    "\n",
    "留 $P$ 法验证（Leave-p-out）：有 $N$ 个样本，将每 $P$ 个样本作为测试样本，其它 $N-P$ 个样本作为训练样本，这样得到个 train-test pairs，不像 LeaveOneOut 和 KFold，当 $P>1$ 时，测试集将会发生重叠，当 $P=1$ 的时候，就变成了留一法\n",
    "\n",
    "###  leaveOneOut：测试集就留下一个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T14:46:06.352026Z",
     "start_time": "2018-09-02T14:46:06.343594Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeaveOneOut()\n",
      "Train Index: [1 2 3 4 5] ,Test Index: [0]\n",
      "Train Index: [0 2 3 4 5] ,Test Index: [1]\n",
      "Train Index: [0 1 3 4 5] ,Test Index: [2]\n",
      "Train Index: [0 1 2 4 5] ,Test Index: [3]\n",
      "Train Index: [0 1 2 3 5] ,Test Index: [4]\n",
      "Train Index: [0 1 2 3 4] ,Test Index: [5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]])\n",
    "y = np.array([1, 2, 3, 4, 5, 6])\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)\n",
    "print(loo)\n",
    "for train_index, test_index in loo.split(X, y):\n",
    "    print(\"Train Index:\", train_index, \",Test Index:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    # print(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeavePOut：测试集留下P个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T14:46:53.004629Z",
     "start_time": "2018-09-02T14:46:52.991234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeavePOut(p=3)\n",
      "Train Index: [3 4 5] ,Test Index: [0 1 2]\n",
      "Train Index: [2 4 5] ,Test Index: [0 1 3]\n",
      "Train Index: [2 3 5] ,Test Index: [0 1 4]\n",
      "Train Index: [2 3 4] ,Test Index: [0 1 5]\n",
      "Train Index: [1 4 5] ,Test Index: [0 2 3]\n",
      "Train Index: [1 3 5] ,Test Index: [0 2 4]\n",
      "Train Index: [1 3 4] ,Test Index: [0 2 5]\n",
      "Train Index: [1 2 5] ,Test Index: [0 3 4]\n",
      "Train Index: [1 2 4] ,Test Index: [0 3 5]\n",
      "Train Index: [1 2 3] ,Test Index: [0 4 5]\n",
      "Train Index: [0 4 5] ,Test Index: [1 2 3]\n",
      "Train Index: [0 3 5] ,Test Index: [1 2 4]\n",
      "Train Index: [0 3 4] ,Test Index: [1 2 5]\n",
      "Train Index: [0 2 5] ,Test Index: [1 3 4]\n",
      "Train Index: [0 2 4] ,Test Index: [1 3 5]\n",
      "Train Index: [0 2 3] ,Test Index: [1 4 5]\n",
      "Train Index: [0 1 5] ,Test Index: [2 3 4]\n",
      "Train Index: [0 1 4] ,Test Index: [2 3 5]\n",
      "Train Index: [0 1 3] ,Test Index: [2 4 5]\n",
      "Train Index: [0 1 2] ,Test Index: [3 4 5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import LeavePOut\n",
    "X=np.array([[1,2],[3,4],[5,6],[7,8],[9,10],[11,12]])\n",
    "y=np.array([1,2,3,4,5,6])\n",
    "lpo=LeavePOut(p=3)\n",
    "lpo.get_n_splits(X)\n",
    "print(lpo)\n",
    "for train_index,test_index in lpo.split(X,y):\n",
    "    print(\"Train Index:\",train_index,\",Test Index:\",test_index)\n",
    "    X_train,X_test=X[train_index],X[test_index]\n",
    "    y_train,y_test=y[train_index],y[test_index]\n",
    "    #print(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集划分方法——随机划分法：ShuffleSplit，GroupShuffleSplit，StratifiedShuffleSplit\n",
    "\n",
    "- ShuffleSplit 迭代器产生指定**数量的独立**的 $train/test$ 数据集划分，首先对样本全体随机打乱，然后再划分出 $train/test$ 对，可以使用随机数种子random_state来控制数字序列发生器使得讯算结果可重现\n",
    "- ShuffleSplit 是 KFlod 交叉验证的比较好的替代，他允许更好的**控制迭代次数和 $train/test$ 的样本比例**\n",
    "- StratifiedShuffleSplit 和 ShuffleSplit 的一个变体，返回分层划分，也就是在创建划分的时候要**保证每一个划分中类的样本比例与整体数据集中的原始比例保持一致**\n",
    "\n",
    "### ShuffleSplit \n",
    "\n",
    "把数据集打乱顺序，然后划分测试集和训练集，训练集额和测试集的比例随机选定，训练集和测试集的比例的和可以小于 $1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T14:50:41.857283Z",
     "start_time": "2018-09-02T14:50:41.847859Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(n_splits=3, random_state=0, test_size=0.25, train_size=None)\n",
      "Train Index: [1 3 0 4] ,Test Index: [5 2]\n",
      "Train Index: [4 0 2 5] ,Test Index: [1 3]\n",
      "Train Index: [1 2 4 0] ,Test Index: [3 5]\n",
      "==============================\n",
      "ShuffleSplit(n_splits=3, random_state=0, test_size=0.25, train_size=0.5)\n",
      "Train Index: [1 3 0] ,Test Index: [5 2]\n",
      "Train Index: [4 0 2] ,Test Index: [1 3]\n",
      "Train Index: [1 2 4] ,Test Index: [3 5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "X=np.array([[1,2],[3,4],[5,6],[7,8],[9,10],[11,12]])\n",
    "y=np.array([1,2,3,4,5,6])\n",
    "rs=ShuffleSplit(n_splits=3,test_size=.25,random_state=0)\n",
    "rs.get_n_splits(X)\n",
    "print(rs)\n",
    "for train_index,test_index in rs.split(X,y):\n",
    "    print(\"Train Index:\",train_index,\",Test Index:\",test_index)\n",
    "    X_train,X_test=X[train_index],X[test_index]\n",
    "    y_train,y_test=y[train_index],y[test_index]\n",
    "    #print(X_train,X_test,y_train,y_test)\n",
    "print(\"==============================\")\n",
    "rs=ShuffleSplit(n_splits=3,train_size=.5,test_size=.25,random_state=0)\n",
    "rs.get_n_splits(X)\n",
    "print(rs)\n",
    "for train_index,test_index in rs.split(X,y):\n",
    "    print(\"Train Index:\",train_index,\",Test Index:\",test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StratifiedShuffleSplitShuffleSplit \n",
    "\n",
    "把数据集打乱顺序，然后划分测试集和训练集，训练集额和测试集的比例随机选定，训练集和测试集的比例的和可以小于 $1$,但是还要保证训练集中各类所占的比例是一样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T14:52:06.835842Z",
     "start_time": "2018-09-02T14:52:06.819954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedShuffleSplit(n_splits=3, random_state=0, test_size=0.5,\n",
      "            train_size=None)\n",
      "Train Index: [5 4 1] ,Test Index: [3 2 0]\n",
      "Train Index: [5 2 3] ,Test Index: [0 4 1]\n",
      "Train Index: [5 0 4] ,Test Index: [3 1 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]])\n",
    "y = np.array([1, 2, 1, 2, 1, 2])\n",
    "sss = StratifiedShuffleSplit(n_splits=3, test_size=.5, random_state=0)\n",
    "sss.get_n_splits(X, y)\n",
    "print(sss)\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    print(\"Train Index:\", train_index, \",Test Index:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    # print(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理\n",
    "\n",
    "我们为什么要进行数据预处理？\n",
    "\n",
    "通常，真实生活中，我们获得的数据中往往存在很多的无用信息，甚至存在错误信息，而机器学习中有一句话叫做\"Garbage in，Garbage out\"，数据的健康程度对于算法结果的影响极大。数据预处理就是让那些冗余混乱的源数据变得能满足其应用要求。\n",
    "\n",
    "当然，仅仅是数据预处理的方法就可以写好几千字的文章了，在这里只谈及几个基础的数据预处理的方法。\n",
    "\n",
    "skleran 中为我们提供了一个数据预处理的 package：`preprocessing`，我们直接导入即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T14:26:02.695135Z",
     "start_time": "2018-09-02T14:26:02.691631Z"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn.preprocessing as sk_preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的例子我们使用:`[[1, -1, 2], [0, 2, -1], [0, 1, -2]]`做为初始数据。\n",
    "    \n",
    "## 数据的归一化\n",
    "\n",
    "基于 `mean` 和 `std` 的标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T14:27:04.823452Z",
     "start_time": "2018-09-02T14:27:04.818466Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基于mean和std的标准化: [[ 5.83777663e-01  5.78490274e-01 -4.46390200e-01 -5.78622166e-01\n",
      "  -4.59626499e-01]\n",
      " [ 1.78208748e+00  1.82365655e+00 -1.49369334e+00 -1.75732374e+00\n",
      "  -1.53002699e+00]\n",
      " [ 1.42364786e-01 -4.34864195e-01  1.19857094e+00 -2.31182824e-01\n",
      "   1.15469921e+00]\n",
      " [-1.08616319e+00 -1.31141585e+00  1.36421794e+00  1.03980354e+00\n",
      "   1.37225487e+00]\n",
      " [-3.42107391e-01  4.16121589e-04 -5.08928171e-01  3.92171245e-01\n",
      "  -4.77218310e-01]\n",
      " [-1.07995935e+00 -6.56282899e-01 -1.13777169e-01  1.13515394e+00\n",
      "  -6.00822798e-02]]\n"
     ]
    }
   ],
   "source": [
    "scaler = sk_preprocessing.StandardScaler().fit(X)\n",
    "new_X = scaler.transform(X)\n",
    "print('基于mean和std的标准化:',new_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "规范化到一定区间内 `feature_range` 为数据规范化的范围"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T14:27:32.567318Z",
     "start_time": "2018-09-02T14:27:32.561862Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "规范化到一定区间内 [[0.5822158  0.60282695 0.36645754 0.40750585 0.36881342]\n",
      " [1.         1.         0.         0.         0.        ]\n",
      " [0.4283196  0.27959535 0.94203914 0.52762409 0.92503979]\n",
      " [0.         0.         1.         0.96703505 1.        ]\n",
      " [0.25941101 0.41843754 0.34457514 0.74313278 0.36275205]\n",
      " [0.00216293 0.208969   0.4828408  1.         0.50647897]]\n"
     ]
    }
   ],
   "source": [
    "scaler = sk_preprocessing.MinMaxScaler(feature_range=(0,1)).fit(X)\n",
    "new_X=scaler.transform(X)\n",
    "print('规范化到一定区间内',new_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据的正则化\n",
    "\n",
    "首先求出样本的 $p$-范数，然后该样本的所有元素都要除以该范数，这样最终使得每个样本的范数都为 $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T14:28:39.266627Z",
     "start_time": "2018-09-02T14:28:39.258195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "求二范数 [[ 0.28390667  0.40858817 -0.5817849  -0.55374853 -0.3276303 ]\n",
      " [ 0.30681812  0.4104597  -0.52876131 -0.60647922 -0.29996653]\n",
      " [ 0.18754123 -0.0872681   0.76378216 -0.45815483  0.40489941]\n",
      " [-0.30549798 -0.41042697  0.53205791  0.60342151  0.30169115]\n",
      " [-0.08310828  0.19296775 -0.83443596  0.24293007 -0.4478371 ]\n",
      " [-0.36426189 -0.16911862 -0.39503282  0.80230951 -0.19738463]]\n"
     ]
    }
   ],
   "source": [
    "new_X = sk_preprocessing.normalize(X,norm='l2')\n",
    "print('求二范数',new_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分类器\n",
    "\n",
    "大致可以将这些分类器分成两类： \n",
    "\n",
    "- 单一分类器\n",
    "- 集成分类器\n",
    "\n",
    "## 单一分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T15:23:24.553955Z",
     "start_time": "2018-09-02T14:54:46.452075Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KN \t-->  1.0\n",
      "SVC \t-->  0.21240344622697563\n",
      "DT \t-->  0.2783125371360666\n",
      "RF \t-->  0.966131907308378\n",
      "ET \t-->  1.0\n",
      "AB \t-->  0.03608437314319667\n",
      "GB \t-->  0.05460487225193108\n",
      "GNB \t-->  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LD \t-->  1.0\n",
      "QD \t-->  1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# meta-estimator\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "\n",
    "classifiers = {\n",
    "    'KN': KNeighborsClassifier(3),\n",
    "    'SVC': SVC(kernel=\"linear\", C=0.025),\n",
    "    'SVC': SVC(gamma=2, C=1),\n",
    "    'DT': DecisionTreeClassifier(max_depth=5),\n",
    "    'RF': RandomForestClassifier(n_estimators=10, max_depth=5, max_features=1),  # clf.feature_importances_\n",
    "    'ET': ExtraTreesClassifier(n_estimators=10, max_depth=None),  # clf.feature_importances_\n",
    "    'AB': AdaBoostClassifier(n_estimators=100),\n",
    "    'GB': GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0), # clf.feature_importances_\n",
    "    'GNB': GaussianNB(),\n",
    "    'LD': LinearDiscriminantAnalysis(),\n",
    "    'QD': QuadraticDiscriminantAnalysis()}\n",
    "\n",
    "    \n",
    "    \n",
    "X, y = make_blobs(n_samples=10000, n_features=10, centers=100, random_state=0)\n",
    "\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    scores = cross_val_score(clf, X, y)\n",
    "    print(name,'\\t--> ',scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T15:23:24.625368Z",
     "start_time": "2018-09-02T15:23:24.620408Z"
    }
   },
   "source": [
    "## 集成分类器\n",
    "\n",
    "集成分类器有四种：Bagging, Voting, GridSearch, PipeLine。最后一个 PipeLine其实是管道技术\n",
    "\n",
    "### Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T15:23:24.632312Z",
     "start_time": "2018-09-02T15:02:28.078Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "meta_clf = KNeighborsClassifier() \n",
    "bg_clf = BaggingClassifier(meta_clf, max_samples=0.5, max_features=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T15:23:24.635785Z",
     "start_time": "2018-09-02T15:02:29.730Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, 1:3], iris.target\n",
    "\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard', weights=[2,1,2])\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\n",
    "    scores = cross_validation.cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T15:23:24.638265Z",
     "start_time": "2018-09-02T15:02:35.284Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "\n",
    "# 生成数据\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# 元分类器\n",
    "meta_clf = RandomForestClassifier(n_estimators=20)\n",
    "\n",
    "# =================================================================\n",
    "# 设置参数\n",
    "param_dist = {\"max_depth\": [3, None],\n",
    "              \"max_features\": sp_randint(1, 11),\n",
    "              \"min_samples_split\": sp_randint(1, 11),\n",
    "              \"min_samples_leaf\": sp_randint(1, 11),\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# 运行随机搜索 RandomizedSearch\n",
    "n_iter_search = 20\n",
    "rs_clf = RandomizedSearchCV(meta_clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search)\n",
    "\n",
    "start = time()\n",
    "rs_clf.fit(X, y)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "print(rs_clf.grid_scores_)\n",
    "\n",
    "# =================================================================\n",
    "# 设置参数\n",
    "param_grid = {\"max_depth\": [3, None],\n",
    "              \"max_features\": [1, 3, 10],\n",
    "              \"min_samples_split\": [1, 3, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# 运行网格搜索 GridSearch\n",
    "gs_clf = GridSearchCV(meta_clf, param_grid=param_grid)\n",
    "start = time()\n",
    "gs_clf.fit(X, y)\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(gs_clf.grid_scores_)))\n",
    "print(gs_clf.grid_scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PipeLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T15:23:24.642232Z",
     "start_time": "2018-09-02T15:02:39.447Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.datasets import samples_generator\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# 生成数据\n",
    "X, y = samples_generator.make_classification(n_informative=5, n_redundant=0, random_state=42)\n",
    "\n",
    "# 定义Pipeline，先方差分析，再SVM\n",
    "anova_filter = SelectKBest(f_regression, k=5)\n",
    "clf = svm.SVC(kernel='linear')\n",
    "pipe = Pipeline([('anova', anova_filter), ('svc', clf)])\n",
    "\n",
    "# 设置anova的参数k=10，svc的参数C=0.1（用双下划线\"__\"连接！）\n",
    "pipe.set_params(anova__k=10, svc__C=.1)\n",
    "pipe.fit(X, y)\n",
    "\n",
    "prediction = pipe.predict(X)\n",
    "\n",
    "pipe.score(X, y)                        \n",
    "\n",
    "# 得到 anova_filter 选出来的特征\n",
    "s = pipe.named_steps['anova'].get_support()\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
