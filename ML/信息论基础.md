## 信息论基础

定义一个事件 $X=x$ 的**自信息**为

$$
I(x) = -\log P(x)
$$

**信息熵**简称**熵**, 是表示随机变量不确定性的度量. 定义为

$$
H(X) = \mathbb{E}_{X \sim P}[I(x)] = - \mathbb{E}_{X \sim P} [\log P(x)]
$$

也可记作 $H(P)$. 信息熵越大包含的信息就越多, 那么随机变量的不确定性就越大.

**条件熵**定义为

$$
H(Y|X) = \mathbb{E}_{X\times Y \sim P}[I(x|y)]
$$

定理: $H(Y|X) \leq H(Y)$.

### 互信息

**互信息**, 也称为**信息增益**, 是描述两个随机变量之间的相关程度, 也就是给定一个随机变量 $X$ 后, 另一个随机变量 $Y$ 不确定性的削减程度, 记作

$$
I(X,Y) = H(Y) - H(Y|X)
$$

互信息的性质:

- 由于 $H(Y) \geq H(Y|X)$, 所以 $0 \leq I(X,Y) \leq H(Y)$.
- 当随机变量 $X$ 与 $Y$ 完全相关时, $H(Y|X) = 0$, 且 $I(X, Y)$ 取得最大值.
- 当随机变量 $X$ 与 $Y$ 完全无关时, $H(Y|X) = H(Y)$, 且 $I(X, Y)$ 取得最小值.

在决策树算法中, 互信息被用来作为特征选择的一种度量标准, 给定训练数据集 $D$, 每个数据集都由 $n$ 维特征构成, 构建决策树时, 一个核心问题是采用哪一个特征来划分数据集? 每个特征可以看作一个随机变量, $n$ 维特征可以记作 $(X_2,X_2, \cdots, X_n)$

一种合理的选择方案是, 分别计算 $I(D,X_i)$, 计算第 $i$ 维特征与训练数据集 $D$ d 相关性, I(D,X_i)$ 越大, 说明第 $i$ 维特征与训练数据集 $D$ d 越相关, 也就是第 $i$ 维特征的练数据包含数据集 $D$ 的信息越多.

**相对熵**, 全称 Kullback-Leibler Divergence, 也被称为** KL 散度**, KL 距离, 定义为:

$$
D_{KL}(P||Q) = \mathbb{E}_{X\sim P} [\log \frac{P(x)}{Q(x)}] = \mathbb{E}_{X\sim P} [\log P(x)- \log Q(x)]
$$

KL 散度常用来衡量两个分布的差异.

**交叉熵**: $H(P, Q) = H(P) + D_{KL}(P||Q)$, 即

$$
H(P, Q) = - \mathbb{E}_{X\sim P} [\log Q(x)]
$$

因而, 针对 $Q$ 最小化交叉熵等价于最小化 KL 散度.