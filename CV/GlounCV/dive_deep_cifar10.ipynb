{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-06T09:36:19.878112Z",
     "start_time": "2018-10-06T09:36:19.522023Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Dive Deep into Training with CIFAR10\n",
    "==============================================\n",
    "\n",
    "Hope you enjoyed playing with our demo script.\n",
    "Now, you may be wandering: how exactly was the model trained?\n",
    "In this tutorial, we will focus on answering this question.\n",
    "\n",
    "Prerequisites\n",
    "-------------\n",
    "\n",
    "We assume readers have a basic understanding of ``Gluon``.\n",
    "If not, we suggest you spend 60 minutes to get started with the [Gluon Crash\n",
    "Course](http://gluon-crash-course.mxnet.io/index.html).\n",
    "\n",
    "As we all know, training deep neural networks on GPUs is way faster than\n",
    "training on CPU.\n",
    "In the previous tutorials, we used CPU because classifying a single image is a\n",
    "relatively easy task.\n",
    "However, since we are about to train a model, it is strongly recommended to\n",
    "use a machine with GPU(s).\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>The rest of the tutorial walks you through the details of ``CIFAR10`` training.\n",
    "    If you want a quick start without knowing the details, try downloading\n",
    "    this script and start training with just one command.\n",
    "\n",
    "    :download:`Download train_cifar10.py<../../../scripts/classification/cifar/train_cifar10.py>`\n",
    "\n",
    "    Here's a sample command with recommended parameters:\n",
    "\n",
    "    ::\n",
    "\n",
    "        python train_cifar10.py --num-epochs 240 --mode hybrid --num-gpus 1 -j 8 --batch-size 128            --wd 0.0001 --lr 0.1 --lr-decay 0.1 --lr-decay-epoch 80,160 --model cifar_resnet20_v1</p></div>\n",
    "\n",
    "\n",
    "Network Structure\n",
    "-----------------\n",
    "\n",
    "First, let's import the necessary libraries into python.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-06T09:38:09.844990Z",
     "start_time": "2018-10-06T09:37:52.778012Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import argparse, time, logging, random, math\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "\n",
    "from mxnet import gluon, nd\n",
    "from mxnet import autograd as ag\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "\n",
    "from gluoncv.model_zoo import get_model\n",
    "from gluoncv.utils import makedirs, TrainingHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are numerous structures for convolutional neural networks.\n",
    "Here we pick a simple yet well-performing structure, ``cifar_resnet20_v1``, for the\n",
    "tutorial.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-06T09:38:16.130146Z",
     "start_time": "2018-10-06T09:38:09.853940Z"
    }
   },
   "outputs": [],
   "source": [
    "# number of GPUs to use\n",
    "num_gpus = 1\n",
    "ctx = [mx.gpu(i) for i in range(num_gpus)]\n",
    "\n",
    "# Get the model CIFAR_ResNet20_v1, with 10 output classes, without pre-trained weights\n",
    "net = get_model('cifar_resnet20_v1', classes=10)\n",
    "net.initialize(mx.init.Xavier(), ctx = ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Augmentation and Data Loader\n",
    "---------------------------------\n",
    "\n",
    "Data augmentation is a common technique used for training. It is\n",
    "base on the assumption that, for the same object, photos under different\n",
    "composition, lighting condition, or color should all yield the same prediction.\n",
    "\n",
    "Here are photos of the Golden Bridge, taken by many people,\n",
    "at different time from different angles.\n",
    "We can easily tell that they are photos of the same thing.\n",
    "\n",
    "|image-golden-bridge|\n",
    "\n",
    "We want to teach this invariance to our model, by playing \"augmenting\"\n",
    "input image. Our augmentation transforms the image with\n",
    "resizing, cropping, flipping and other techniques.\n",
    "\n",
    "With ``Gluon``, we can create our transform function as following:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-06T09:50:23.623479Z",
     "start_time": "2018-10-06T09:50:23.614499Z"
    }
   },
   "outputs": [],
   "source": [
    "class Transforms(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "        # 针对训练数据做处理\n",
    "        self.train = transforms.Compose([\n",
    "            # Randomly crop an area, and then resize it to be 32x32\n",
    "            transforms.RandomResizedCrop(32),\n",
    "            # Randomly flip the image horizontally\n",
    "            transforms.RandomFlipLeftRight(),\n",
    "            # Randomly jitter the brightness, contrast and saturation of the image\n",
    "            transforms.RandomColorJitter(\n",
    "                brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "            # Randomly adding noise to the image\n",
    "            transforms.RandomLighting(0.1),\n",
    "            # Transpose the image from height*width*num_channels to num_channels*height*width\n",
    "            # and map values from [0, 255] to [0,1]\n",
    "            transforms.ToTensor(),\n",
    "            # Normalize the image with mean and standard deviation calculated across all images\n",
    "            transforms.Normalize([0.4914, 0.4822, 0.4465],\n",
    "                                 [0.2023, 0.1994, 0.2010])\n",
    "        ])\n",
    "        # 针对测试数据做处理\n",
    "        self.test = transforms.Compose([\n",
    "            transforms.Resize(32),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.4914, 0.4822, 0.4465],\n",
    "                                 [0.2023, 0.1994, 0.2010])\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-06T09:50:48.670048Z",
     "start_time": "2018-10-06T09:50:48.665060Z"
    }
   },
   "outputs": [],
   "source": [
    "T = Transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-06T09:51:02.478780Z",
     "start_time": "2018-10-06T09:51:02.474759Z"
    }
   },
   "outputs": [],
   "source": [
    "T.train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that most of the operations are randomized. This in effect\n",
    "increases the number of different images the model sees during training.\n",
    "The more data we have, the better our model generalizes over\n",
    "unseen images.\n",
    "\n",
    "On the other hand, when making prediction, we would like to remove all\n",
    "random operations in order to get a deterministic result. The transform\n",
    "function for prediction is:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it is important to keep the normalization step, since the\n",
    "model only works well on inputs from the same distribution.\n",
    "\n",
    "With the transform functions, we can define data loaders for our\n",
    "training and validation datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Size for Each GPU\n",
    "per_device_batch_size = 128\n",
    "# Number of data loader workers\n",
    "num_workers = 8\n",
    "# Calculate effective total batch size\n",
    "batch_size = per_device_batch_size * num_gpus\n",
    "\n",
    "# Set train=True for training data\n",
    "# Set shuffle=True to shuffle the training data\n",
    "train_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR10(train=True).transform_first(transform_train),\n",
    "    batch_size=batch_size, shuffle=True, last_batch='discard', num_workers=num_workers)\n",
    "\n",
    "# Set train=False for validation data\n",
    "val_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR10(train=False).transform_first(transform_test),\n",
    "    batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer, Loss and Metric\n",
    "--------------------------\n",
    "\n",
    "Optimizer improves the model during training. Here we use the popular\n",
    "Nesterov accelerated gradient descent algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate decay factor\n",
    "lr_decay = 0.1\n",
    "# Epochs where learning rate decays\n",
    "lr_decay_epoch = [80, 160, np.inf]\n",
    "\n",
    "# Nesterov accelerated gradient descent\n",
    "optimizer = 'nag'\n",
    "# Set parameters\n",
    "optimizer_params = {'learning_rate': 0.1, 'wd': 0.0001, 'momentum': 0.9}\n",
    "\n",
    "# Define our trainer for net\n",
    "trainer = gluon.Trainer(net.collect_params(), optimizer, optimizer_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, ``lr_decay`` and ``lr_decay_epoch`` are not directly\n",
    "used in ``trainer``. One important idea in model training is to\n",
    "gradually decrease learning rate. This means the optimizer takes large\n",
    "steps at the beginning, but step size becomes smaller and smaller in time.\n",
    "\n",
    "Our plan sets the learning rate to 0.1 at the beginning, then\n",
    "divide it by 10 at the 80-th epoch, then again at the 160-th epoch.\n",
    "We'll use `lr_decay_epoch` in the main training loop for this purpose.\n",
    "\n",
    "In order to optimize our model, we need a loss function.\n",
    "In essence, loss functions compute the difference between predictions and the\n",
    "ground-truth as a measure of model performance.\n",
    "We can then take the gradients of the loss w.r.t. the weights.\n",
    "Gradients points the optimizer to the direction weights should move to\n",
    "improve model performance.\n",
    "\n",
    "For classification tasks, we usually use softmax cross entropy as the\n",
    "loss function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics are similar to loss functions, but they are different in the\n",
    "following aspects:\n",
    "\n",
    "-  Metric is how we evaluate model performance. Each metric is related to a\n",
    "   specific task, but independent from the model training process.\n",
    "-  For classification, we usually only use one loss function to train\n",
    "   our model, but we can have several metrics for evaluating\n",
    "   performance.\n",
    "-  Loss function can be used as a metric, but sometimes its values are hard\n",
    "   to interpretate. For instance, the concept \"accuracy\" is\n",
    "   easier to understand than \"softmax cross entropy\"\n",
    "\n",
    "For simplicity, we use accuracy as the metric to monitor our training\n",
    "process. Besides, we record metric values, and will print them at the\n",
    "end of training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metric = mx.metric.Accuracy()\n",
    "train_history = TrainingHistory(['training-error', 'validation-error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation\n",
    "----------\n",
    "\n",
    "Validation dataset provides us a way of monitoring the training process.\n",
    "We have labels for validation data, but they are held out during training.\n",
    "Instead, we use them to evaluate the models performance on unseen data\n",
    "and prevent overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(ctx, val_data):\n",
    "    metric = mx.metric.Accuracy()\n",
    "    for i, batch in enumerate(val_data):\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "        outputs = [net(X) for X in data]\n",
    "        metric.update(label, outputs)\n",
    "    return metric.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate performance, we need a metric. Then, we loop\n",
    "through the validation data and predict with our model.\n",
    "We'll run this function at the end of every epoch to show improvement.\n",
    "over the last epoch.\n",
    "\n",
    "Training\n",
    "--------\n",
    "\n",
    "After all the preparations, we can finally start training!\n",
    "Following is the script.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>In order to finish the tutorial quickly, we only train for 3 epochs.\n",
    "  In your experiments, we recommend setting ``epochs=240``.</p></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "lr_decay_count = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    tic = time.time()\n",
    "    train_metric.reset()\n",
    "    train_loss = 0\n",
    "\n",
    "    # Learning rate decay\n",
    "    if epoch == lr_decay_epoch[lr_decay_count]:\n",
    "        trainer.set_learning_rate(trainer.learning_rate*lr_decay)\n",
    "        lr_decay_count += 1\n",
    "\n",
    "    # Loop through each batch of training data\n",
    "    for i, batch in enumerate(train_data):\n",
    "        # Extract data and label\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "\n",
    "        # AutoGrad\n",
    "        with ag.record():\n",
    "            output = [net(X) for X in data]\n",
    "            loss = [loss_fn(yhat, y) for yhat, y in zip(output, label)]\n",
    "\n",
    "        # Backpropagation\n",
    "        for l in loss:\n",
    "            l.backward()\n",
    "\n",
    "        # Optimize\n",
    "        trainer.step(batch_size)\n",
    "\n",
    "        # Update metrics\n",
    "        train_loss += sum([l.sum().asscalar() for l in loss])\n",
    "        train_metric.update(label, output)\n",
    "\n",
    "    name, acc = train_metric.get()\n",
    "    # Evaluate on Validation data\n",
    "    name, val_acc = test(ctx, val_data)\n",
    "\n",
    "    # Update history and print metrics\n",
    "    train_history.update([1-acc, 1-val_acc])\n",
    "    print('[Epoch %d] train=%f val=%f loss=%f time: %f' %\n",
    "        (epoch, acc, val_acc, train_loss, time.time()-tic))\n",
    "\n",
    "# We can plot the metric scores with:\n",
    "\n",
    "train_history.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you trained the model for 240 epochs, the plot may look like:\n",
    "\n",
    "|image-aug|\n",
    "\n",
    "We can better observe the process of model training with plots.\n",
    "For example, one may ask what will happen if there's no data augmentation:\n",
    "\n",
    "|image-no-aug|\n",
    "\n",
    "We can see that training error is much lower than validation error.\n",
    "After the model reaches 100\\% accuracy on training data,\n",
    "it stops improving on validation data.\n",
    "These two plots evidently demonstrates the importance of data augmentation.\n",
    "\n",
    "Model Saving and Loading\n",
    "------------------------\n",
    "\n",
    "After training, we usually want to save it for later use.\n",
    "This is simply done with:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.save_parameters('dive_deep_cifar10_resnet20_v2.params')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next time if you need to use it, just run\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_params('dive_deep_cifar10_resnet20_v2.params', ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Step\n",
    "---------\n",
    "\n",
    "This is the end of our adventure with ``CIFAR10``, but there are many\n",
    "more datasets and algorithms in computer vision!\n",
    "\n",
    "If you would like to know how to train a model on a much larger dataset\n",
    "than ``CIFAR10``, e.g. ImageNet, please read `ImageNet Training <dive_deep_imagenet.html>`__.\n",
    "\n",
    "Or, if you want like to know what can be done with the model you just\n",
    "trained, please read the tutorial on `Transfer learning <transfer_learning_minc.html>`__.\n",
    "\n",
    ".. |image-no-aug| image:: https://raw.githubusercontent.com/dmlc/web-data/master/gluoncv/classification/overfitting.png\n",
    ".. |image-aug| image:: https://raw.githubusercontent.com/dmlc/web-data/master/gluoncv/classification/normal_training.png\n",
    ".. |image-golden-bridge| image:: https://raw.githubusercontent.com/dmlc/web-data/master/gluoncv/classification/golden-bridge.png\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
