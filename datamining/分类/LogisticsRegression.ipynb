{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T13:45:45.620141Z",
     "start_time": "2019-03-01T13:45:38.896900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(683, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample code number</th>\n",
       "      <th>Clump Thickness</th>\n",
       "      <th>Uniformity of Cell Size</th>\n",
       "      <th>Uniformity of Cell Shape</th>\n",
       "      <th>Marginal Adhesion</th>\n",
       "      <th>Single Epithelial Cell Size</th>\n",
       "      <th>Bare Nuclei</th>\n",
       "      <th>Bland Chromatin</th>\n",
       "      <th>Normal Nucleoli</th>\n",
       "      <th>Mitoses</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000025</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002945</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1015425</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1016277</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1017023</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sample code number  Clump Thickness  Uniformity of Cell Size  \\\n",
       "0             1000025                5                        1   \n",
       "1             1002945                5                        4   \n",
       "2             1015425                3                        1   \n",
       "3             1016277                6                        8   \n",
       "4             1017023                4                        1   \n",
       "\n",
       "   Uniformity of Cell Shape  Marginal Adhesion  Single Epithelial Cell Size  \\\n",
       "0                         1                  1                            2   \n",
       "1                         4                  5                            7   \n",
       "2                         1                  1                            2   \n",
       "3                         8                  1                            3   \n",
       "4                         1                  3                            2   \n",
       "\n",
       "  Bare Nuclei  Bland Chromatin  Normal Nucleoli  Mitoses  Class  \n",
       "0           1                3                1        1      2  \n",
       "1          10                3                2        1      2  \n",
       "2           2                3                1        1      2  \n",
       "3           4                3                7        1      2  \n",
       "4           1                3                1        1      2  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入pandas与numpy工具包。\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 创建特征列表。\n",
    "column_names = [\n",
    "    'Sample code number', 'Clump Thickness', 'Uniformity of Cell Size',\n",
    "    'Uniformity of Cell Shape', 'Marginal Adhesion',\n",
    "    'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin',\n",
    "    'Normal Nucleoli', 'Mitoses', 'Class'\n",
    "]\n",
    "\n",
    "# 使用pandas.read_csv函数从互联网读取指定数据。\n",
    "data = pd.read_csv(\n",
    "    'https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data',\n",
    "    names=column_names)\n",
    "\n",
    "# 将?替换为标准缺失值表示。\n",
    "data = data.replace(to_replace='?', value=np.nan)\n",
    "# 丢弃带有缺失值的数据（只要有一个维度有缺失）。\n",
    "data = data.dropna(how='any')\n",
    "\n",
    "# 输出data的数据量和维度。\n",
    "print(data.shape)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集的简介见：[breast-cancer-wisconsin](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.names)\n",
    "\n",
    "我们得知该数据集共有 $699$ 个样本，$11$ 个特征，其中一列特征表示检索的 id, $9$ 列与肿瘤相关的医学特征以及一列表示肿瘤类型的数值。`2` 表示 benign (良性), `4` 表示 malignant (恶性). 该数据集中有 $16$ 个缺失值，并使用 `?` 标出。\n",
    "\n",
    "## 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T13:46:36.175054Z",
     "start_time": "2019-03-01T13:46:33.429944Z"
    }
   },
   "outputs": [],
   "source": [
    "# 使用sklearn.cross_valiation里的train_test_split模块用于分割数据。\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 随机采样25%的数据用于测试，剩下的75%用于构建训练集合。\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data[column_names[1:10]],\n",
    "    data[column_names[10]],\n",
    "    test_size=0.25,\n",
    "    random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T13:46:36.333979Z",
     "start_time": "2019-03-01T13:46:36.326949Z"
    }
   },
   "outputs": [],
   "source": [
    "# 转换数据类型\n",
    "X_train = X_train.astype(float)\n",
    "X_test = X_test.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T13:46:36.550026Z",
     "start_time": "2019-03-01T13:46:36.538941Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    344\n",
       "4    168\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查验训练样本的数量和类别分布。\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T13:46:51.810290Z",
     "start_time": "2019-03-01T13:46:51.802071Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    100\n",
       "4     71\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查验测试样本的数量和类别分布。\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集的标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T13:46:54.030702Z",
     "start_time": "2019-03-01T13:46:53.911675Z"
    }
   },
   "outputs": [],
   "source": [
    "# 从sklearn.preprocessing里导入StandardScaler。\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# 从sklearn.linear_model里导入LogisticRegression与SGDClassifier。\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# 标准化数据，保证每个维度的特征数据方差为1，均值为0。使得预测结果不会被某些维度过大的特征值而主导。\n",
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用 sklearn 进行  Logistics Regression 学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T13:50:23.881537Z",
     "start_time": "2019-03-01T13:50:23.877832Z"
    }
   },
   "outputs": [],
   "source": [
    "# 初始化LogisticRegression与SGDClassifier。\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "sgdc = SGDClassifier(max_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T13:50:26.923509Z",
     "start_time": "2019-03-01T13:50:24.552370Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.89 ms ± 141 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# 调用LogisticRegression中的fit函数/模块用来训练模型参数。\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T13:50:32.366660Z",
     "start_time": "2019-03-01T13:50:27.016347Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "652 µs ± 19.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# 调用SGDClassifier中的fit函数/模块用来训练模型参数。\n",
    "sgdc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T13:50:34.422146Z",
     "start_time": "2019-03-01T13:50:34.418016Z"
    }
   },
   "outputs": [],
   "source": [
    "# 使用训练好的模型lr对X_test进行预测，结果储存在变量lr_y_predict中。\n",
    "lr_y_predict = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T13:50:38.052995Z",
     "start_time": "2019-03-01T13:50:38.047954Z"
    }
   },
   "outputs": [],
   "source": [
    "# 使用训练好的模型sgdc对X_test进行预测，结果储存在变量sgdc_y_predict中。\n",
    "sgdc_y_predict = sgdc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T13:50:41.176296Z",
     "start_time": "2019-03-01T13:50:41.163528Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LR Classifier: 0.9883040935672515\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.99      0.99      0.99       100\n",
      "   Malignant       0.99      0.99      0.99        71\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       171\n",
      "   macro avg       0.99      0.99      0.99       171\n",
      "weighted avg       0.99      0.99      0.99       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 从sklearn.metrics里导入classification_report模块。\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 使用逻辑斯蒂回归模型自带的评分函数score获得模型在测试集上的准确性结果。\n",
    "print('Accuracy of LR Classifier:', lr.score(X_test, y_test))\n",
    "# 利用classification_report模块获得LogisticRegression其他三个指标的结果。\n",
    "print(\n",
    "    classification_report(\n",
    "        y_test, lr_y_predict, target_names=['Benign', 'Malignant']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T13:50:44.147487Z",
     "start_time": "2019-03-01T13:50:44.135256Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuarcy of SGD Classifier: 0.9824561403508771\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       1.00      0.97      0.98       100\n",
      "   Malignant       0.96      1.00      0.98        71\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       171\n",
      "   macro avg       0.98      0.98      0.98       171\n",
      "weighted avg       0.98      0.98      0.98       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用随机梯度下降模型自带的评分函数score获得模型在测试集上的准确性结果。\n",
    "print('Accuarcy of SGD Classifier:', sgdc.score(X_test, y_test))\n",
    "# 利用classification_report模块获得SGDClassifier其他三个指标的结果。\n",
    "print(\n",
    "    classification_report(\n",
    "        y_test, sgdc_y_predict, target_names=['Benign', 'Malignant']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般说来，`SGDClassifier` 训练速度快于 `LogisticRegression`，适用于 $10$ 万量级的数据；而 `LogisticRegression` 的参数是采用精确解析的方式,模型的精度会相对高一点, 但是对于 $10$ 万量级的数据内存开销很大，有点不切实际。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---------------------\n",
    "\n",
    "# 使用 MXNet 实现 Logistics Regression\n",
    "\n",
    "使用 SGD 进行优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T13:53:35.396765Z",
     "start_time": "2019-03-01T13:53:35.388651Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import mxnet as mx\n",
    "from mxnet import autograd, nd\n",
    "\n",
    "\n",
    "def data_iter(batch_size, features, labels, trainable=True):\n",
    "    '''\n",
    "    数据迭代器\n",
    "    '''\n",
    "    num_examples = len(features)\n",
    "    indices = np.arange(num_examples)\n",
    "    if trainable:\n",
    "        random.shuffle(indices)  # 样本的读取顺序是随机的。\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        j = np.array(indices[i: min(i + batch_size, num_examples)])\n",
    "        # take 函数根据索引返回对应元素。\n",
    "        yield features.take(j, axis=0), labels.take(j, axis=0)\n",
    "\n",
    "\n",
    "def LR(X, w, b):\n",
    "    '''\n",
    "    Logistics Regression\n",
    "    '''\n",
    "    z = nd.dot(X, w) + b\n",
    "    exp = 1 / (1 + z.exp())\n",
    "    return exp\n",
    "\n",
    "\n",
    "def squared_loss(y_hat, y):\n",
    "    '''\n",
    "    损失函数\n",
    "    '''\n",
    "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2\n",
    "\n",
    "\n",
    "def sgd(params, lr, batch_size):\n",
    "    '''\n",
    "    优化算法\n",
    "    '''\n",
    "    for param in params:\n",
    "        param[:] -= lr * param.grad / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T13:53:36.369670Z",
     "start_time": "2019-03-01T13:53:36.363826Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_features = X_train.shape[1]\n",
    "w = nd.random.normal(shape=(num_features, 1))+0.001\n",
    "b = nd.zeros(1)\n",
    "w.attach_grad()\n",
    "b.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T13:53:40.267793Z",
     "start_time": "2019-03-01T13:53:37.065666Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 2.928271\n",
      "epoch 2, loss 2.019408\n",
      "epoch 3, loss 2.892815\n",
      "epoch 4, loss 2.237970\n",
      "epoch 5, loss 1.769795\n",
      "epoch 6, loss 2.129934\n",
      "epoch 7, loss 1.562507\n",
      "epoch 8, loss 1.538613\n",
      "epoch 9, loss 1.913836\n",
      "epoch 10, loss 2.418076\n",
      "epoch 11, loss 2.273044\n",
      "epoch 12, loss 2.389298\n",
      "epoch 13, loss 1.602880\n",
      "epoch 14, loss 2.368069\n",
      "epoch 15, loss 2.102364\n",
      "epoch 16, loss 1.581749\n",
      "epoch 17, loss 2.730835\n",
      "epoch 18, loss 1.956262\n",
      "epoch 19, loss 1.312663\n",
      "epoch 20, loss 1.436876\n",
      "epoch 21, loss 1.688527\n",
      "epoch 22, loss 1.813171\n",
      "epoch 23, loss 1.555527\n",
      "epoch 24, loss 1.425814\n",
      "epoch 25, loss 2.059360\n",
      "epoch 26, loss 1.929702\n",
      "epoch 27, loss 1.800970\n",
      "epoch 28, loss 2.179071\n",
      "epoch 29, loss 2.050675\n",
      "epoch 30, loss 2.048612\n",
      "epoch 31, loss 1.160229\n",
      "epoch 32, loss 1.285836\n",
      "epoch 33, loss 2.423917\n",
      "epoch 34, loss 1.789774\n",
      "epoch 35, loss 2.294211\n",
      "epoch 36, loss 1.787175\n",
      "epoch 37, loss 1.786389\n",
      "epoch 38, loss 1.784949\n",
      "epoch 39, loss 2.289278\n",
      "epoch 40, loss 1.530488\n",
      "epoch 41, loss 1.656279\n",
      "epoch 42, loss 1.781740\n",
      "epoch 43, loss 2.159589\n",
      "epoch 44, loss 2.537048\n",
      "epoch 45, loss 1.779509\n",
      "epoch 46, loss 1.526620\n",
      "epoch 47, loss 2.030429\n",
      "epoch 48, loss 2.029480\n",
      "epoch 49, loss 2.281081\n",
      "epoch 50, loss 1.776345\n",
      "epoch 51, loss 2.027870\n",
      "epoch 52, loss 2.405266\n",
      "epoch 53, loss 1.648761\n",
      "epoch 54, loss 1.522425\n",
      "epoch 55, loss 1.773729\n",
      "epoch 56, loss 1.899312\n",
      "epoch 57, loss 1.521302\n",
      "epoch 58, loss 2.150189\n",
      "epoch 59, loss 2.023935\n",
      "epoch 60, loss 2.149402\n",
      "epoch 61, loss 1.897242\n",
      "epoch 62, loss 2.148483\n",
      "epoch 63, loss 1.519098\n",
      "epoch 64, loss 2.021993\n",
      "epoch 65, loss 1.895866\n",
      "epoch 66, loss 2.146925\n",
      "epoch 67, loss 1.643648\n",
      "epoch 68, loss 2.020668\n",
      "epoch 69, loss 1.768875\n",
      "epoch 70, loss 1.894281\n",
      "epoch 71, loss 1.893967\n",
      "epoch 72, loss 1.390944\n",
      "epoch 73, loss 1.893371\n",
      "epoch 74, loss 1.893257\n",
      "epoch 75, loss 1.767249\n",
      "epoch 76, loss 1.767027\n",
      "epoch 77, loss 1.641145\n",
      "epoch 78, loss 2.143412\n",
      "epoch 79, loss 1.389419\n",
      "epoch 80, loss 2.017380\n",
      "epoch 81, loss 2.770958\n",
      "epoch 82, loss 2.016926\n",
      "epoch 83, loss 1.639871\n",
      "epoch 84, loss 2.016599\n",
      "epoch 85, loss 1.765151\n",
      "epoch 86, loss 2.016029\n",
      "epoch 87, loss 2.015920\n",
      "epoch 88, loss 1.764692\n",
      "epoch 89, loss 2.015599\n",
      "epoch 90, loss 1.764267\n",
      "epoch 91, loss 2.015231\n",
      "epoch 92, loss 1.889495\n",
      "epoch 93, loss 1.763771\n",
      "epoch 94, loss 1.763661\n",
      "epoch 95, loss 1.386953\n",
      "epoch 96, loss 1.386866\n",
      "epoch 97, loss 1.637667\n",
      "epoch 98, loss 1.888604\n",
      "epoch 99, loss 2.013896\n",
      "epoch 100, loss 1.888328\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "num_epochs = 100\n",
    "batch_size = 64\n",
    "\n",
    "for epoch in range(num_epochs):  # 训练模型一共需要 num_epochs 个迭代周期。\n",
    "    # 在一个迭代周期中，使用训练数据集中所有样本一次（假设样本数能够被批量大小整除）。\n",
    "    # X 和 y 分别是小批量样本的特征和标签。\n",
    "    for X, y in data_iter(32, X_train, y_train):\n",
    "        X = nd.array(X)/255\n",
    "        y = nd.array(y)\n",
    "        with autograd.record():\n",
    "            y_hat = LR(X, w, b)\n",
    "            L = squared_loss(y_hat, y)\n",
    "        L.backward()\n",
    "        sgd([w, b], lr, batch_size)  # 使用小批量随机梯度下降迭代模型参数。\n",
    "    train_l = squared_loss(LR(X, w, b), y)\n",
    "    print('epoch %d, loss %f' % (epoch + 1, train_l.mean().asnumpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T13:53:43.259805Z",
     "start_time": "2019-03-01T13:53:43.254802Z"
    }
   },
   "outputs": [],
   "source": [
    "LR_y_predict = LR(nd.array(X_test), w, b).asnumpy()\n",
    "LR_y_predict = np.where(LR_y_predict > .5, 2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T13:53:45.545478Z",
     "start_time": "2019-03-01T13:53:45.531477Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       1.00      0.34      0.51        71\n",
      "   Malignant       0.68      1.00      0.81       100\n",
      "\n",
      "   micro avg       0.73      0.73      0.73       171\n",
      "   macro avg       0.84      0.67      0.66       171\n",
      "weighted avg       0.81      0.73      0.68       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 从sklearn.metrics里导入classification_report模块。\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\n",
    "    classification_report(\n",
    "        y_test<3, LR_y_predict<3, target_names=['Benign', 'Malignant']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T13:53:46.621521Z",
     "start_time": "2019-03-01T13:53:46.612640Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0.1 0.2]\n",
       "<NDArray 2 @cpu(0)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = nd.array([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\n",
    "y = nd.array([0, 1])\n",
    "nd.pick(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T13:53:50.268278Z",
     "start_time": "2019-03-01T13:53:50.261133Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0.1 0.3 0.6]\n",
       " [0.3 0.2 0.5]]\n",
       "<NDArray 2x3 @cpu(0)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
