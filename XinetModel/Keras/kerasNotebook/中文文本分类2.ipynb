{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上接 [中文文本分类1](http://www.cnblogs.com/q735613050/p/9135789.html)\n",
    "\n",
    "# 使用朴素贝叶斯进行分类\n",
    "![](http://static.zybuluo.com/xinet/6043hb7nsr5n6c9jt2ltguzt/%E6%8D%95%E8%8E%B7.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('E:/xinlib')\n",
    "from base.filename import FileName, nameBunch\n",
    "import base.file as file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'art': 248,\n",
       " 'computer': 200,\n",
       " 'economic': 325,\n",
       " 'education': 220,\n",
       " 'environment': 201,\n",
       " 'medical': 204,\n",
       " 'military': 249,\n",
       " 'politics': 505,\n",
       " 'sports': 450,\n",
       " 'traffic': 214}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = 'D:/MLBook/chapter02/train_corpus_small/'\n",
    "\n",
    "f = FileName(root)\n",
    "set_dict = f.target_bunch.size_dict    # 获取每个类别的样本数\n",
    "set_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据均衡处理\n",
    "\n",
    "由上可知，数据集出现不均衡的现象，因此需要做数据均衡处理，我的做法是：\n",
    "- 先找出每个类别的样本数，取最小值为 `size`\n",
    "- 然后，在每个类别下随机选取 `size` 个样本作为训练集进行训练\n",
    "- 最后，将训练集按比例随机划分为训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSegment(FileName):\n",
    "    '''\n",
    "    中文分词\n",
    "    \n",
    "    属性\n",
    "    =====\n",
    "    root::未分词分类语料库目录，例如：\"./train_corpus_small/\"\n",
    "    stopword_path::停用词路径，例如：\"E:/Data/中文语料库/hlt_stop_words.txt\"\n",
    "    '''\n",
    "    def __init__(self, root, **kwargs):\n",
    "        super().__init__(root, **kwargs)\n",
    "        self.stopword_path = \"E:/Data/中文语料库/hlt_stop_words.txt\"\n",
    "        self.seg_root = 'D:/seg_corpus/'\n",
    "        self.train_dict, self.val_dict = self.split_dict()\n",
    "        \n",
    "    def make_seg_dir(self, seg_name):\n",
    "        '''\n",
    "        seg_name::分词后分类语料库目录，例如：\"train_corpus_seg/\"\n",
    "        '''\n",
    "        seg_dir = [file.make_dir(self.seg_root + seg_name, dir_name) for dir_name in self.target_bunch.names]\n",
    "        return seg_dir\n",
    "            \n",
    "    def save_seg(self, target_dir):\n",
    "        for file_path in os.listdir(target_dir):               # 遍历类别目录下文件\n",
    "            fullname = target_dir + file_path        # 拼出文件名全路径\n",
    "            content = read_file(fullname).strip()     # 读取文件内容\n",
    "            content = content.replace(\"\\r\\n\", \"\")     # 删除换行和多余的空格\n",
    "            content_seg = jieba.cut(content.strip())       # 为文件内容分词\n",
    "            # 将处理后的文件保存到分词后语料目录\n",
    "            save_file(self.seg_dir +  file_path, \" \".join(content_seg))\n",
    "        \n",
    "    def file_list(self):\n",
    "        '''\n",
    "        Return\n",
    "        =======\n",
    "        seg_dir::分词后的目录\n",
    "        '''\n",
    "        catelist = os.listdir(self.root)\n",
    "        file_names = []\n",
    "        for i, target_name in enumerate(self.target_names):\n",
    "            class_dir = self.root + target_name + \"/\"    # 拼出分类子目录\n",
    "            file_list = os.listdir(class_dir)        # 获取 class_path 下的所有文件\n",
    "            file_names.append([class_dir + name for name in file_list])\n",
    "        return file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = CSegment(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = cs.make_seg_dir('train/')\n",
    "val_dir = cs.make_seg_dir('val/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in cs.target_bunch.names:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in os.listdir(target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dB = cs.balancedBunch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:/MLBook/chapter02/train_corpus_small/art'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dB.name_dict['art']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segment:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.stopword_path = \"E:/Data/中文语料库/hlt_stop_words.txt\"\n",
    "\n",
    "    def get_seg(self, corpus_path):\n",
    "        '''\n",
    "        获取每个目录下所有的文件 mydir in catelist\n",
    "        catelist = os.listdir(corpus_path)\n",
    "\n",
    "        参数\n",
    "        =====\n",
    "        corpus_path::未分词分类语料库路径，例如：\"train_corpus_small/\"\n",
    "        seg_path::分词后分类语料库路径，例如：\"train_corpus_seg/\"\n",
    "\n",
    "        Return\n",
    "        =======\n",
    "        train_corpus_seg, val__corpus_seg::分词后的目录\n",
    "        '''\n",
    "        corpus_path = self.root + corpus_path\n",
    "        train_corpus_seg = self.root + \"train_corpus_seg/\"\n",
    "        catelist = os.listdir(corpus_path)\n",
    "        start = time()\n",
    "        for i, class_dir in enumerate(catelist):\n",
    "            class_path = corpus_path + class_dir + \"/\"    # 拼出分类子目录的路径\n",
    "            file_list = os.listdir(class_path)        # 获取 class_path 下的所有文件\n",
    "            train_dir = make_seg_dir(train_corpus_seg, class_dir)\n",
    "            save_seg(class_path, file_list, train_dir)\n",
    "            if i == 0:\n",
    "                print('  完成语料分词的类别依次为：')\n",
    "            print('\\t%i: %s' % (i, class_dir))\n",
    "        print('--' * 20)\n",
    "        print(\"总计花费时间 %g 秒，中文语料分词结束！！！\" % (time() - start))\n",
    "        return train_corpus_seg\n",
    "\n",
    "    def create_bunch(self, wordbag_path, seg_path):\n",
    "        '''\n",
    "        获取每个目录下所有的文件 mydir in catelist\n",
    "        catelist = os.listdir(corpus_path)\n",
    "\n",
    "        参数\n",
    "        =====\n",
    "        wordbag_path::分词语料 Bunch 对象持久化路径，例如：\"train_word_bag/train_set.dat\"\n",
    "        seg_path::分词后分类语料库路径，例如：\"train_corpus_seg/\"\n",
    "        '''\n",
    "        wordbag_dir = self.root + 'word_bag/'\n",
    "        if not os.path.exists(wordbag_dir):           # 是否存在目录，如果没有创建\n",
    "            os.makedirs(wordbag_dir)\n",
    "        wordbag_path = wordbag_dir + wordbag_path\n",
    "        catelist = os.listdir(seg_path)\n",
    "        bunch = Bunch(target_name=[], label=[], filenames=[], contents=[])\n",
    "        bunch.target_name.extend(catelist)        # 将类别信息保存到 Bunch 对象\n",
    "        start = time()\n",
    "        for i, mydir in enumerate(catelist):\n",
    "            class_path = seg_path + mydir + \"/\"    # 拼出分类子目录的路径\n",
    "            file_list = os.listdir(class_path)        # 获取 class_path 下的所有文件\n",
    "            for k, file_path in enumerate(file_list):               # 遍历类别目录下文件\n",
    "                fullname = class_path + file_path        # 拼出文件名全路径\n",
    "                bunch.label.append(mydir)                # 保存当前文件的分类标签\n",
    "                bunch.filenames.append(fullname)          # 保存当前文件的文件路径\n",
    "                bunch.contents.append(read_file(fullname).strip())    # 保存文件词向量\n",
    "                if k == 0 and i == 0:\n",
    "                    print('构建文本对象中......')\n",
    "                    print('--' * 20)\n",
    "            if i == 0:\n",
    "                print('  文本对象构建的类别依次为：')\n",
    "            print('\\t%i: %s' % (i, mydir))\n",
    "        print('--' * 20)\n",
    "        # 对象持久化\n",
    "        with open(wordbag_path, \"wb\") as file_obj:\n",
    "            pickle.dump(bunch, file_obj)\n",
    "        print(\"总计花费时间 %g 秒，构建文本对象结束！！！\" % (time() - start))\n",
    "        print(\"\")\n",
    "        return wordbag_path\n",
    "\n",
    "    def read_stopword(self):\n",
    "        '''\n",
    "        读取停用词表\n",
    "        \n",
    "        示例\n",
    "        =======\n",
    "        stopword_path = \"E:/Data/中文语料库/hlt_stop_words.txt\"\n",
    "        '''\n",
    "        return read_file(self.stopword_path).splitlines()\n",
    "\n",
    "    def train_tfidf(self, space_name, seg_path):\n",
    "        '''\n",
    "        参数\n",
    "        ======\n",
    "        stopword_name::停用词\n",
    "        seg_path::bunch 分词路径\n",
    "        space_name::词向量词袋\n",
    "        '''\n",
    "        start = time()\n",
    "        space_path = self.root + 'word_bag/' + space_name\n",
    "        \n",
    "        # 1. 读取停用词表\n",
    "        stpwrdlst = self.read_stopword()\n",
    "\n",
    "        # 2. 导入分词后的词向量 bunch 对象\n",
    "        bunch = read_bunch(seg_path)\n",
    "        # 3. 构建 tf-idf 词向量空间对象\n",
    "        tfidfspace = Bunch(target_name=bunch.target_name, label=bunch.label,\n",
    "                           filenames=bunch.filenames, tdm=[], vocabulary={})  #\n",
    "\n",
    "        # 4. 使用 TfidfVectorizer 初始化向量空间模型\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            stop_words=stpwrdlst, sublinear_tf=True, max_df=0.5)\n",
    "        transformer = TfidfTransformer()  # 该类会统计每个词语的 tf-idf 权值\n",
    "\n",
    "        # 文本转为词频矩阵,单独保存字典文件\n",
    "        tfidfspace.tdm = vectorizer.fit_transform(bunch.contents)\n",
    "        tfidfspace.vocabulary = vectorizer.vocabulary_\n",
    "\n",
    "        # 创建词袋的持久化\n",
    "        write_bunch(space_path, tfidfspace)\n",
    "        print(\"花费时间：%g 秒，TF-IDF 词向量空间创建成功！！！\" % (time() - start))\n",
    "        return space_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from segment import Word2Vector, Segment, read_bunch # 参考「中文文本分类1」\n",
    "\n",
    "\n",
    "root = 'D:/MLBook/chapter02/'\n",
    "# 分词\n",
    "S = Segment(root)\n",
    "corpus_path = \"train_corpus_small/\"\n",
    "corpus_seg_dir = S.get_seg(corpus_path)\n",
    "corpus_seg_dir "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 创建 Bunch 对象\n",
    "train_name = \"corpus_seg.dat\" \n",
    "train_path = S.create_bunch(train_name, corpus_seg_dir)\n",
    "train_bunch = read_bunch(train_path)\n",
    "\n",
    "train_path, val_path, train_target, val_target = \\\n",
    "train_test_split(train_bunch.filenames, train_bunch.label, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "WV = Word2Vector(root)\n",
    "train_space_name = \"train_tfdifspace.dat\"          # 词向量词袋保存路径     \n",
    "val_space_name = \"val_tfdifspace.dat\"          \n",
    "train_space_path = WV.train_tfidf(train_space_name, train_path)\n",
    "val_space_path = WV.train_tfidf(val_space_name, val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = read_bunch(train_space_path)\n",
    "val_set = read_bunch(val_space_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入多项式贝叶斯算法包\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha 越小，迭代次数越多，精度越高\n",
    "\n",
    "clf = MultinomialNB(alpha = 0.001).fit(train_set.tdm, train_set.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测分类结果\n",
    "predicted = clf.predict(val_set.tdm)\n",
    "total = len(predicted)\n",
    "rate = 0\n",
    "for flabel, file_name, expct_cate in zip(val_set.label, val_set.filenames, predicted):\n",
    "    if flabel != expct_cate:\n",
    "        rate += 1\n",
    "        print(file_name, '：实际类别：', flabel, '--> 预测类别：', expct_cate)\n",
    "        \n",
    "# 精度\n",
    "print('error rate：', float(rate) * 100 / flaot(total), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 划分数据集\n",
    "X, Y = np.array(list(trainset.values())).T\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size = .2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
