{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**文本挖掘**（Text Mining）是从非结构化文本信息中获取用户感兴趣或者有用的模式的过程。\n",
    "\n",
    "----\n",
    "**文本挖掘是指从大量文本数据中抽取事先未知的、可理解的、最终可用的知识的过程，同时运用这些知识更好地组织信息以便将来参考**。\n",
    "\n",
    "----\n",
    "\n",
    "![](http://static.zybuluo.com/xinet/0fxx6wolrm5o6d110puajhnj/%E6%8D%95%E8%8E%B7.PNG)\n",
    "![](http://static.zybuluo.com/xinet/df32nu4sh06ec5fvy7xr8efv/%E6%8D%95%E8%8E%B7.PNG)\n",
    "\n",
    "*************\n",
    "\n",
    "![](http://static.zybuluo.com/xinet/7a6maijmvsv35dfq0zocds0z/%E6%8D%95%E8%8E%B7.PNG)\n",
    "\n",
    "------\n",
    "\n",
    "# 文本预处理\n",
    "\n",
    "文本处理的核心任务是把非结构化和半结构化的文本转换为结构化的形式，即向量空间模型。\n",
    "\n",
    "具体步骤：\n",
    "\n",
    "## 1. 选择处理的文本范围\n",
    "\n",
    "选择恰当的范围取决于文本挖掘任务的目标：\n",
    "- 对于**分类**或**聚类**的任务，往往把整个文档作为处理单位；\n",
    "- 对于**情感分析**、**文档自动摘要**或**信息检索**，段落或章节可能更合适。\n",
    "\n",
    "## 2. 建立分类文本语库\n",
    "\n",
    "- 训练集语料：已经分好类的文本资源。\n",
    "- 测试集语料：待分类的文本语料，可以是训练集的一部分，也可以是外部来源的文本语料。\n",
    "\n",
    "### 中文语料库\n",
    "- [中文文本语料库整理(不定时更新2015-10-24).md](https://www.jianshu.com/p/206caa232ded)\n",
    "- [THCHS-30：一个免费的中文语料库](https://blog.csdn.net/sut_wj/article/details/70662181)\n",
    "   - 论文下载：[THCHS-30 : A Free Chinese Speech Corpus](https://arxiv.org/pdf/1512.01882v2.pdf)\n",
    "   \n",
    "![](http://static.zybuluo.com/xinet/tz0dt63peabw8p8penn4fc6l/%E6%8D%95%E8%8E%B7.PNG)\n",
    "![](http://static.zybuluo.com/xinet/4hx1jwsfomqpbrrtzwpwecob/%E6%8D%95%E8%8E%B7.PNG)\n",
    "\n",
    "## 3. 文本格式转换\n",
    "![](http://static.zybuluo.com/xinet/ex1476h7onbipic9uz4y3bc0/%E6%8D%95%E8%8E%B7.PNG)\n",
    "![](http://static.zybuluo.com/xinet/a2kda0g6n9znh8cr34ai3k8y/%E6%8D%95%E8%8E%B7.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "import os\n",
    "import time\n",
    "from lxml import etree, html\n",
    "from sklearn.datasets.base import Bunch\n",
    "import pickle\n",
    "from time import time\n",
    "\n",
    "root = 'D:/MLBook' + '/chapter02'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "百度百科_百度百科(function(){window.PDC={_timing:{},_opt:{sample:0.01},_analyzer:{loaded:false,url:\"http://s\n"
     ]
    }
   ],
   "source": [
    "# htm文件路径，以及读取文件\n",
    "path = root + \"/1.htm\"\n",
    "with open(path, \"rb\") as fp:\n",
    "    content = fp.read().decode()\n",
    "    \n",
    "page = html.document_fromstring(content) # 解析文件\n",
    "text = page.text_content() # 去除所有标签\n",
    "print(text[:100]) # 输出去除标签后解析结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 检测句子边界：标记句子的结束\n",
    "\n",
    "![](http://static.zybuluo.com/xinet/larsrx7thi5e3e2mt14ik7qv/%E6%8D%95%E8%8E%B7.PNG)\n",
    "![](http://static.zybuluo.com/xinet/yw4l8pbjkvkghpq1fc4h31h8/%E6%8D%95%E8%8E%B7.PNG)\n",
    "\n",
    "完全解决中文分词的算法是基于概率图模型的**条件随机场**（CRF）\n",
    "![](http://static.zybuluo.com/xinet/m787sbnhgzeyqtj6nj8wq1td/%E6%8D%95%E8%8E%B7.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\q7356\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.992 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Mode: 小明 1995 年 毕业 于 北京 清华大学\n",
      "小明  1995  年  毕业  于  北京  清华大学\n",
      "Full Mode: 小/ 明/ 1995/ 年/ 毕业/ 于/ 北京/ 清华/ 清华大学/ 华大/ 大学\n",
      "小明/  硕士/  毕业/  于/  中国/  科学/  学院/  科学院/  中国科学院/  计算/  计算所/  ，/  后/  在/  日本/  京都/  大学/  日本京都大学/  深造\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "seg_list = jieba.cut(\"小明1995年毕业于北京清华大学\", cut_all=False)\n",
    "print(\"Default Mode:\", \" \".join(seg_list))  # 默认模式\n",
    "\n",
    "seg_list = jieba.cut(\"小明1995年毕业于北京清华大学\")\n",
    "print(\"  \".join(seg_list))\n",
    "\n",
    "seg_list = jieba.cut(\"小明1995年毕业于北京清华大学\", cut_all=True)\n",
    "print(\"Full Mode:\", \"/ \".join(seg_list))  # 全模式\n",
    "\n",
    "seg_list = jieba.cut_for_search(\"小明硕士毕业于中国科学院计算所，\\\n",
    "后在日本京都大学深造\")  # 搜索引擎模式\n",
    "print(\"/  \".join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "小明  终于  在  1995  年  从  北京  清华大学  毕业  了  。\n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.cut(\"小明终于在1995年从北京清华大学毕业了。\")\n",
    "print(\"  \".join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['art',\n",
       " 'computer',\n",
       " 'economic',\n",
       " 'education',\n",
       " 'environment',\n",
       " 'medical',\n",
       " 'military',\n",
       " 'politics',\n",
       " 'sports',\n",
       " 'traffic']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(root+ '/train_corpus_seg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segment:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "    \n",
    "    def save_file(self, save_path, content):\n",
    "        '''\n",
    "        保存文件\n",
    "        '''\n",
    "        with open(save_path, 'wb') as fp:\n",
    "            fp.write(content.encode())\n",
    "            \n",
    "    def read_file(self, path):\n",
    "        with open(path, 'rb') as fp:\n",
    "            content = fp.read().decode()\n",
    "        return content\n",
    "    \n",
    "    def get_seg(self, corpus_path, seg_path):\n",
    "        '''\n",
    "        获取每个目录下所有的文件 mydir in catelist\n",
    "        catelist = os.listdir(corpus_path)\n",
    "\n",
    "        参数\n",
    "        =====\n",
    "        corpus_path::未分词分类语料库路径，例如：\"train_corpus_small/\"\n",
    "        seg_path::分词后分类语料库路径，例如：\"train_corpus_seg/\"\n",
    "        '''\n",
    "        corpus_path = self.root + corpus_path\n",
    "        seg_path = self.root + seg_path\n",
    "        catelist = os.listdir(corpus_path)\n",
    "        start = time()\n",
    "        for i, mydir in enumerate(catelist):\n",
    "            class_path = corpus_path + mydir + \"/\"    # 拼出分类子目录的路径\n",
    "            seg_dir = seg_path + mydir + \"/\"          # 拼出分词后语料分类目录\n",
    "            if not os.path.exists(seg_dir):           # 是否存在目录，如果没有创建\n",
    "                os.makedirs(seg_dir)\n",
    "            file_list = os.listdir(class_path)        # 获取 class_path 下的所有文件\n",
    "            for k, file_path in enumerate(file_list):               # 遍历类别目录下文件\n",
    "                fullname = class_path + file_path        # 拼出文件名全路径\n",
    "                content = self.read_file(fullname).strip()     # 读取文件内容\n",
    "                content = content.replace(\"\\r\\n\", \"\")     # 删除换行和多余的空格\n",
    "                content_seg = jieba.cut(content.strip())       # 为文件内容分词\n",
    "                self.save_file(seg_dir + file_path, \" \".join(content_seg))  # 将处理后的文件保存到分词后语料目录\n",
    "                if k == 0 and i == 0:\n",
    "                    print('保存分词后语料中......')\n",
    "                    print('--' * 20)\n",
    "            if i == 0:\n",
    "                print('  完成语料分词的类别依次为：')\n",
    "            print('\\t%i: %s'%(i, mydir))\n",
    "        print('--' * 20)\n",
    "        print(\"总计花费时间 %g 秒，中文语料分词结束！！！\"%(time() - start))\n",
    "        \n",
    "    def get_bunch(self, wordbag_path, seg_path):\n",
    "        '''\n",
    "        获取每个目录下所有的文件 mydir in catelist\n",
    "        catelist = os.listdir(corpus_path)\n",
    "\n",
    "        参数\n",
    "        =====\n",
    "        wordbag_path::分词语料 Bunch 对象持久化路径，例如：\"train_word_bag/train_set.dat\"\n",
    "        seg_path::分词后分类语料库路径，例如：\"train_corpus_seg/\"\n",
    "        '''\n",
    "        wordbag_path = self.root + wordbag_path\n",
    "        seg_path = self.root + seg_path\n",
    "        catelist = os.listdir(seg_path)\n",
    "        bunch = Bunch(target_name=[], label=[], filenames=[], contents=[])\n",
    "        bunch.target_name.extend(catelist)        # 将类别信息保存到 Bunch 对象\n",
    "        start = time()\n",
    "        for i, mydir in enumerate(catelist):\n",
    "            class_path = seg_path + mydir + \"/\"    # 拼出分类子目录的路径\n",
    "            file_list = os.listdir(class_path)        # 获取 class_path 下的所有文件\n",
    "            for k, file_path in enumerate(file_list):               # 遍历类别目录下文件\n",
    "                fullname = class_path + file_path        # 拼出文件名全路径\n",
    "                bunch.label.append(mydir)                # 保存当前文件的分类标签\n",
    "                bunch.filenames.append(fullname)          # 保存当前文件的文件路径\n",
    "                bunch.contents.append(self.read_file(fullname).strip())    # 保存文件词向量\n",
    "                if k == 0 and i == 0:\n",
    "                    print('构建文本对象中......')\n",
    "                    print('--' * 20)\n",
    "            if i == 0:\n",
    "                print('  文本对象构建的类别依次为：')\n",
    "            print('\\t%i: %s'%(i, mydir))\n",
    "        print('--' * 20)\n",
    "        # 对象持久化                                                                                              \n",
    "        with open(wordbag_path, \"wb\") as file_obj:\n",
    "            pickle.dump(bunch, file_obj)  \n",
    "        print(\"总计花费时间 %g 秒，构建文本对象结束！！！\"%(time() - start))\n",
    "        print(\"\")\n",
    "        return bunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存分词后语料中......\n",
      "----------------------------------------\n",
      "  完成语料分词的类别依次为：\n",
      "\t0: art\n",
      "\t1: computer\n",
      "\t2: economic\n",
      "\t3: education\n",
      "\t4: environment\n",
      "\t5: medical\n",
      "\t6: military\n",
      "\t7: politics\n",
      "\t8: sports\n",
      "\t9: traffic\n",
      "----------------------------------------\n",
      "总计花费时间 19.8525 秒，中文语料分词结束！！！\n"
     ]
    }
   ],
   "source": [
    "root = 'D:/MLBook/chapter02/'\n",
    "corpus_path = \"train_corpus_small/\"  # 未分词分类语料库路径\n",
    "seg_path = \"train_corpus_seg/\"      # 分词后分类语料库路径\n",
    "\n",
    "S = Segment(root)\n",
    "S.get_seg(corpus_path, seg_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在实际应用中，为了后续生成**向量空间模型**的方便，这些分词后的文本信息还要转换为文本向量信息并对象化。这里我们使用 Scikit-Learn 库的 Bunch 数据结构：\n",
    "\n",
    "- Bunch 类提供一种 `(key, value)` 的对象形式\n",
    "- `target_name`：所有分类集名称列表\n",
    "- `label`：每个文件的分类标签列表\n",
    "- `filenames`：文件路径\n",
    "- `contents`：分词后文件词向量形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.base import Bunch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将分好词的文本转换并持久化为 Bunch 类形式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "构建文本对象中......\n",
      "----------------------------------------\n",
      "  文本对象构建的类别依次为：\n",
      "\t0: art\n",
      "\t1: computer\n",
      "\t2: economic\n",
      "\t3: education\n",
      "\t4: environment\n",
      "\t5: medical\n",
      "\t6: military\n",
      "\t7: politics\n",
      "\t8: sports\n",
      "\t9: traffic\n",
      "----------------------------------------\n",
      "总计花费时间 0.891003 秒，构建文本对象结束！！！\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordbag_path = \"train_word_bag/train_set.dat\"  # 分词语料 Bunch 对象持久化路径\n",
    "seg_path = \"train_corpus_seg/\"      # 分词后分类语料库路径\n",
    "\n",
    "S = Segment(root)\n",
    "bunch = S.get_bunch(wordbag_path, seg_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样就在目录下生成了一个 `train_set.dat` 文件。此文件保存了所有训练集文件的所有分类信息，以及每个文件的文件名、文件所属分类和词向量。\n",
    "\n",
    "## Scikit-Learn 简介\n",
    "![](http://static.zybuluo.com/xinet/flhx7hf3y1k516p4evtdljmo/%E6%8D%95%E8%8E%B7.PNG)\n",
    "\n",
    "# 向量空间模型\n",
    "![](http://static.zybuluo.com/xinet/n5tdg3q4wpciqzm8alfwygbi/%E6%8D%95%E8%8E%B7.PNG)\n",
    "\n",
    "\n",
    "\n",
    "为节省存储空间和提高搜索效率，在文本分类之前会自动过滤掉某些字或词，这些字或词被称为**停用词**。这些词一般都是意义模糊的常用词和语气助词，通常它们对文本起不了分类特征的意义。\n",
    "\n",
    "## 权重策略——TF-IDF 方法\n",
    "\n",
    "    文本1：My dog ate my homework\n",
    "    文本2：My cat ate the sandwich\n",
    "    文本3：A dolphin ate the homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a = 'My dog ate my homework '\n",
    "b = 'My cat ate the sandwich '\n",
    "c = 'A dolphin ate the homework '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2list(s):\n",
    "    return s.lower().strip(' ').split(' ')\n",
    "\n",
    "def wordbag(*args):\n",
    "    wb = []\n",
    "    for t in args:\n",
    "        wb.extend(str2list(t))\n",
    "    return set(wb)\n",
    "\n",
    "def word2vec(wb, t):\n",
    "    a2 = []\n",
    "    for k in wb:\n",
    "        if k in str2list(t):\n",
    "            a2.append(1)\n",
    "        else:\n",
    "            a2.append(0)\n",
    "    return a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a', 'ate', 'cat', 'dog', 'dolphin', 'homework', 'my', 'sandwich', 'the'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wb = wordbag(*[a, b, c])\n",
    "wb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将上述文本信息转换为词向量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本1： [1, 1, 0, 0, 1, 0, 0, 1, 0]\n",
      "文本2： [0, 1, 1, 0, 0, 0, 1, 1, 1]\n",
      "文本3： [1, 0, 0, 1, 0, 1, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "a2 = word2vec(wb, a)\n",
    "b2 = word2vec(wb, b)\n",
    "c2 = word2vec(wb, c)\n",
    "\n",
    "print('文本1：', a2)\n",
    "print('文本2：', b2)\n",
    "print('文本3：', c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词频统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_stat(w):\n",
    "    d = {}\n",
    "    for v in str2list(w):\n",
    "        d[v] = d.get(v, 0) + 1\n",
    "    return d\n",
    "\n",
    "def word2vec(wb, t):\n",
    "    L = []\n",
    "    for k in wb:\n",
    "        d = get_word_stat(t)\n",
    "        if k in d.keys():\n",
    "            L.append(d[k])\n",
    "        else:\n",
    "            L.append(0)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本1： [1, 2, 0, 0, 1, 0, 0, 1, 0]\n",
      "文本2： [0, 1, 1, 0, 0, 0, 1, 1, 1]\n",
      "文本3： [1, 0, 0, 1, 0, 1, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "a2 = word2vec(wb, a)    # 'my' 出现了两次\n",
    "b2 = word2vec(wb, b)\n",
    "c2 = word2vec(wb, c)\n",
    "\n",
    "print('文本1：', a2)\n",
    "print('文本2：', b2)\n",
    "print('文本3：', c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对词向量进行归一化，将其转换为**概率分布**："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def TF(t):\n",
    "    a3 = np.array(t)\n",
    "    return a3 / a3.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本1： [ 0.2  0.4  0.   0.   0.2  0.   0.   0.2  0. ]\n",
      "文本2： [ 0.   0.2  0.2  0.   0.   0.   0.2  0.2  0.2]\n",
      "文本3： [ 0.2  0.   0.   0.2  0.   0.2  0.2  0.2  0. ]\n"
     ]
    }
   ],
   "source": [
    "a3 = TF(a2)    # 'my' 出现了两次\n",
    "b3 = TF(b2)\n",
    "c3 = TF(c2)\n",
    "\n",
    "print('文本1：', a3)\n",
    "print('文本2：', b3)\n",
    "print('文本3：', c3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将**词频**信息变成了**概率分布**，这就是文档的 **TF** 信息。\n",
    "![](http://static.zybuluo.com/xinet/457wfzck2a0qmc0pmtac26mr/%E6%8D%95%E8%8E%B7.PNG)\n",
    "![](http://static.zybuluo.com/xinet/4tvhr3443y14z7gj8tyibjav/%E6%8D%95%E8%8E%B7.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec2(wb, t):\n",
    "    L = []\n",
    "    for k in wb:\n",
    "        D = get_word_stat(a + b + c)\n",
    "        d = get_word_stat(t)\n",
    "        if k in d.keys():\n",
    "            L.append(D[k])\n",
    "        else:\n",
    "            L.append(0)\n",
    "    return L\n",
    "\n",
    "def IDF(wb, t):\n",
    "    e = word2vec2(wb, t)\n",
    "    g = 3 / (np.array(e) + 1) \n",
    "    return np.log(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a4 = IDF(wb, a)    \n",
    "b4 = IDF(wb, b)\n",
    "c4 = IDF(wb, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF1 = a3 * a4\n",
    "TFIDF2 = b3 * b4\n",
    "TFIDF3 = c3 * c4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        , -0.11507283,  0.        ,  0.        ,  0.08109302,\n",
       "        0.        ,  0.        , -0.05753641,  0.        ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFIDF1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们使用 Scikit-Learn 包来实现 TF-IDF 算法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    '''\n",
    "    读取停用词表\n",
    "    '''\n",
    "    with open(path, \"rb\") as fp:\n",
    "        content = fp.read()\n",
    "    return content.decode()  # 转换为 string\n",
    "\n",
    "class Word2Vector:\n",
    "    \n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        \n",
    "    def read_bunch(self, path):\n",
    "        '''\n",
    "        读取 bunch 对象\n",
    "        '''\n",
    "        with open(self.root + path, \"rb\") as file_obj:\n",
    "            bunch = pickle.load(file_obj) \n",
    "        return bunch\n",
    "    \n",
    "    def write_bunch(self, path, bunchobj):\n",
    "        '''\n",
    "        写入 bunch 对象\n",
    "        '''\n",
    "        with open(self.root + path, \"wb\") as file_obj:\n",
    "            pickle.dump(bunchobj, file_obj) \n",
    "            \n",
    "    def read_stopword(self, stopword_path):\n",
    "        '''\n",
    "        读取停用词表\n",
    "        \n",
    "        示例\n",
    "        =======\n",
    "        path = \"train_word_bag/hlt_stop_words.txt\"\n",
    "        '''\n",
    "        return read_file(self.root + stopword_path).splitlines()\n",
    "    \n",
    "    def tfidf(self, stopword_path, path, space_path):\n",
    "        '''\n",
    "        参数\n",
    "        ======\n",
    "        stopword_path::停用词路径\n",
    "        path::bunch 保存路径\n",
    "        space_path::词向量词袋保存路径\n",
    "        '''\n",
    "        start = time()\n",
    "        # 1. 读取停用词表\n",
    "        stpwrdlst = self.read_stopword(stopword_path)\n",
    "\n",
    "        # 2. 导入分词后的词向量 bunch 对象\n",
    "        bunch = self.read_bunch(path)\n",
    "\n",
    "        # 3. 构建 tf-idf 词向量空间对象\n",
    "        tfidfspace = Bunch(target_name=bunch.target_name, label=bunch.label, \n",
    "                           filenames=bunch.filenames, tdm=[], vocabulary={})\n",
    "\n",
    "        # 4. 使用 TfidfVectorizer 初始化向量空间模型 \n",
    "        vectorizer = TfidfVectorizer(stop_words=stpwrdlst, sublinear_tf = True, max_df = 0.5)\n",
    "        transformer = TfidfTransformer() # 该类会统计每个词语的 tf-idf 权值\n",
    "\n",
    "        # 文本转为词频矩阵,单独保存字典文件 \n",
    "        tfidfspace.tdm = vectorizer.fit_transform(bunch.contents)\n",
    "        tfidfspace.vocabulary = vectorizer.vocabulary_\n",
    "\n",
    "        # 创建词袋的持久化\n",
    "        self.write_bunch(space_path, tfidfspace)\n",
    "        print(\"花费时间：%g 秒，TF-IDF 词向量空间创建成功！！！\"%(time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction  \n",
    "from sklearn.feature_extraction.text import TfidfTransformer  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "花费时间：1.6075 秒，TF-IDF 词向量空间创建成功！！！\n"
     ]
    }
   ],
   "source": [
    "WV = Word2Vector(root)\n",
    "path = \"train_word_bag/train_set.dat\"        # 词向量空间保存路径\n",
    "stopword_path = \"train_word_bag/hlt_stop_words.txt\"\n",
    "space_path = \"train_word_bag/tfdifspace.dat\"        # 词向量空间保存路径\n",
    "\n",
    "WV.tfidf(stopword_path, path, space_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
