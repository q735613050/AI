{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "学习资料：[ Keras中文文档](http://keras-cn.readthedocs.io/en/latest/)\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 数据处理模块：`keras.preprocessing`\n",
    "包含 `text`、`sequence`、`image` 三个子库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 文字预处理\n",
    "- [文本预处理](http://keras-cn.readthedocs.io/en/latest/preprocessing/text/)\n",
    "\n",
    "在文字建模中，一般都需要把原始文字拆解成单字、单词或词组，然后将这些拆分后的要素进行索引、标记化供机器学习算法使用。这种预处理叫做**标注**（Tokenize）。Keras 提供了现成的方法，既方便，又高效。一般来说，对于已经读入的文字的预处理包含以下几个步骤：\n",
    "1. 文字拆分\n",
    "2. 建立索引\n",
    "3. 序列补奇（Padding）\n",
    "4. 转换为矩阵\n",
    "5. 使用标注类批量处理文本文件\n",
    "\n",
    "所有跟文字相关的预处理函数都在 `keras.preprocessing.txt` 这个子库里。但是这是为英文设计的，如果处理中文，建议使用结巴分词里提供的切分函数 `cut` 来进行文字拆分。\n",
    "\n",
    "### 文字拆分\n",
    "使用 `text_to_word_sequence` 函数，将一段文字根据预定义的分隔符（不能为空值）切分成字符串或者单词（英文）。\n",
    "- 返回一个单词列表，但是会先预处理一下，比如将过滤表中的字符过滤掉，或者将字符都转换为小写字母等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.preprocessing.text as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'Those that come through the Ivory Gate cheat us with empty promises that never see fullfillment.Those that come\\\n",
    "through the Gate of Horn inform the dreamer of truth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['those', 'that', 'come', 'through', 'the', 'ivory']\n"
     ]
    }
   ],
   "source": [
    "out1 = text.text_to_word_sequence(txt)\n",
    "print(out1[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Those', 'that', 'come', 'through', 'the', 'Ivory']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2 = text.text_to_word_sequence(txt, lower= False)\n",
    "out2[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ose', 't', 't', 'come', 't', 'roug']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out3 = text.text_to_word_sequence(txt, lower= False, filters= 'Tha')\n",
    "out3[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 中文分词—— `jieba`\n",
    "- 基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图（DAG）\n",
    "- 采用动态规划查找最大概率路径，找出基于词频的最大切分组合\n",
    "- 对于未登陆词，采用了基于汉字成词能力的 HMM 模型，使用 Viterbi 算法。\n",
    "\n",
    "结巴分词提供了两种分词方法：\n",
    "1. `cut` 对应的返回列表是 `lcut`。\n",
    "2. `cut_for_search` 是为搜索引擎构造索引所采用的比精确分词模式颗粒度略细的分词方法，返回一个可迭代的生成器（Generator）对象，可以使用 `for` 循环来获取分割后的单词。对应的返回列表是 `lcut_for_search`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "chn= '夫医道所兴，其来久矣。上古神农始尝草木而知百药。黄帝咨访岐伯，伯高，少俞之徒，内考五脏六腑，外综经络血气色候，参之天地，验之人物，\\\n",
    "本性命，穷神极变，而针道生焉。其论至妙，雷公受业传之於后。伊尹以亚圣之才，撰用《神农本草》，以为《汤液》。中古名医有俞跗，医缓，扁鹊，\\\n",
    "秦有医和，汉有仓公。其论皆经理识本，非徒诊病而已。汉有华佗，张仲景，华佗奇方异治，施世者多，亦不能尽记其本末。若知直祭酒刘季琰病发於畏恶，\\\n",
    "治之而瘥。云：“后九年季琰病应发，发当有感，仍本於畏恶，病动必死。”终如其言。仲景见侍中王仲宣时年二十余。谓曰：“君有病，四十当眉落，\\\n",
    "眉落半年而死。”令含服五石汤可免。仲宣嫌其言忤，受汤勿服。居三日，仲景见仲宣谓曰：“服汤否？”仲宣曰：“已服。”仲景曰：\\\n",
    "“色候固非服汤之诊，君何轻命也！”仲宣犹不信。后二十年果眉落，后一百八十七日而死，终如其言。此二事虽扁鹊，仓公无以加也。\\\n",
    "华佗性恶矜技，终以戮死。 仲景论广伊尹《汤液》为十数卷，用之多验。近代太医令王叔和撰次仲景遗论甚精，皆可施用。\\\n",
    "按《七略》艺文志，《黄帝内经》十八卷。今有《针经》九卷，《素问》九卷，二九十八卷，即《内经》也。亦有所亡失，其论遐远，\\\n",
    "然称述多而切事少，有不编次。比按仓公传，其学皆出於《素问》，《素问》论病精微，《九卷》是原本经脉，其义深奥，不易览也。\\\n",
    "又有《明堂孔穴针灸治要》，皆黄帝岐伯遗事也。三部同归，文多重複，错互非一。甘露中，吾病风加苦聋百日，方治要皆浅近。\\\n",
    "乃撰集三部，使事类相从，删其浮辞，除其重複，论其精要，至为十二卷。《易》曰：“观其所聚，而天地之情事见矣。”况物理乎。\\\n",
    "事类相从，聚之义也。夫受先人之体，有八尺之躯，而不知医事，此所谓游魂耳！若不精通於於医道，虽有忠孝之心，仁慈之性，君父危困，\\\n",
    "赤子涂地，无以济之，此固圣贤所以精思极论尽其理也。由此言之，焉可忽乎。其本论其文有理，虽不切於近事，不甚删也。若必精要，\\\n",
    "俟其闲暇，当撰核以为教经云尔。'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lcut/cut` 接受三个参数，分别是：需要分割的 Unicode 字符串、分词是否采用细颗粒度模式和是否使用 HMM 模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\xiner\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.135 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "532 ['夫', '医道', '所兴', '，', '其来', '久', '矣', '。']\n"
     ]
    }
   ],
   "source": [
    "chnout1 = jieba.lcut(chn, cut_all= False)\n",
    "print(len(chnout1), chnout1[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "532 ['夫', '医道', '所', '兴', '', '', '其', '来', '久', '矣', '', '', '上古', '古神', '神农']\n"
     ]
    }
   ],
   "source": [
    "chnout2 = jieba.lcut(chn, cut_all= True)\n",
    "print(len(chnout1), chnout2[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "843 ['夫', '医道', '所兴', '，', '其来', '久', '矣', '。', '上古', '神农', '始尝', '草木', '而', '知百药', '。']\n"
     ]
    }
   ],
   "source": [
    "chnout3 = jieba.lcut(chn, cut_all= True, HMM= True)\n",
    "print(len(chnout3), chnout1[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立索引\n",
    "完成分词以后，得到的单字或者单词并不能直接用于建模，还需要将他们转换为数字序号，才能进行后续处理。这就是建立索引。\n",
    "\n",
    "方法：对拆分出来的每个单字或单词，排序之后编号即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('with', 0),\n",
       " ('us', 1),\n",
       " ('truth', 2),\n",
       " ('through', 3),\n",
       " ('those', 4),\n",
       " ('those', 5),\n",
       " ('the', 6),\n",
       " ('the', 7),\n",
       " ('the', 8),\n",
       " ('that', 9),\n",
       " ('that', 10),\n",
       " ('that', 11),\n",
       " ('see', 12),\n",
       " ('promises', 13),\n",
       " ('of', 14),\n",
       " ('of', 15),\n",
       " ('never', 16),\n",
       " ('ivory', 17),\n",
       " ('inform', 18),\n",
       " ('horn', 19),\n",
       " ('gate', 20),\n",
       " ('gate', 21),\n",
       " ('fullfillment', 22),\n",
       " ('empty', 23),\n",
       " ('dreamer', 24),\n",
       " ('comethrough', 25),\n",
       " ('come', 26),\n",
       " ('cheat', 27)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "out1.sort(reverse= True)\n",
    "k = list(zip(out1, np.arange(len(out1))))\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cheat': 27,\n",
       " 'come': 26,\n",
       " 'comethrough': 25,\n",
       " 'dreamer': 24,\n",
       " 'empty': 23,\n",
       " 'fullfillment': 22,\n",
       " 'gate': 21,\n",
       " 'horn': 19,\n",
       " 'inform': 18,\n",
       " 'ivory': 17,\n",
       " 'never': 16,\n",
       " 'of': 15,\n",
       " 'promises': 13,\n",
       " 'see': 12,\n",
       " 'that': 11,\n",
       " 'the': 8,\n",
       " 'those': 5,\n",
       " 'through': 3,\n",
       " 'truth': 2,\n",
       " 'us': 1,\n",
       " 'with': 0}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立索引也可以使用 One Hot 编码法，即对于 $K$ 个不同的单字或者单词，依次设定一个 $1$ 到 $K$ 的数值来索引这 $K$ 个单字或者单词构成的词汇表。使用 `one_hot` 函数很容易实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xin = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
    "tout = (text.text_to_word_sequence(str(xin)))\n",
    "tout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 3, 4, 2, 2, 2, 2, 1, 1, 4, 2, 4, 2]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xout = text.one_hot(str(xin), 5)\n",
    "xout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2\n",
      "1 0 1\n",
      "2 2 3\n",
      "3 3 4\n",
      "4 1 2\n",
      "5 1 2\n",
      "6 1 2\n",
      "7 1 2\n",
      "8 0 1\n",
      "9 0 1\n",
      "10 3 4\n",
      "11 1 2\n",
      "12 3 4\n",
      "13 1 2\n"
     ]
    }
   ],
   "source": [
    "for s in range(len(xin)):\n",
    "    print(s, hash(tout[s])%(5-1), xout[s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`one_hot` 函数有两个参数：一个是待索引的字符串列表；一个是最大索引值 $n$。这个函数将输入的字符串列表按照规则将其分配给 $0,\\cdots, n-1$ 共 $n$ 个索引值之一。\n",
    "\n",
    "一般要将大量不同的数据映射到一个有限的空间中，通常采用的方法是：哈希表，`one_hot` 函数也不例外。\n",
    "\n",
    "### 序列补齐\n",
    "\n",
    "使用 `` 函数，其输入的要素为列表串（list of list）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[1, 2, 3], [4, 5], [6, 7, 8, 9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 3],\n",
       "       [0, 0, 4, 5],\n",
       "       [6, 7, 8, 9]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y0 = pad_sequences(x)\n",
    "y0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 0, 0],\n",
       "       [4, 5, 0, 0, 0],\n",
       "       [6, 7, 8, 9, 0]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1 = pad_sequences(x, maxlen= 5, padding= 'post')\n",
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 0],\n",
       "       [7, 8, 9]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2 = pad_sequences(x, maxlen= 3, padding= 'post')\n",
    "y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [0, 4, 5],\n",
       "       [7, 8, 9]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3 = pad_sequences(x, maxlen= 3, padding= 'pre')\n",
    "y3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 转换为矩阵\n",
    "- `pad_sequences` 方法\n",
    "\n",
    "```python\n",
    "max_sentence_len = 50\n",
    "X = []\n",
    "for sentences in texts:\n",
    "    x = [word_idx(w) for w in sentences]\n",
    "    X.append(x)\n",
    "    \n",
    "pad_sequences(X, maxlen= max_sentence_len)\n",
    "```\n",
    "- 使用标注类 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用标注类批量处理文本文件\n",
    "标注类（Tokenizer class）\n",
    "\n",
    "```python\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(nb_words= 1000)\n",
    "tokenizer.fit_on_text(alltext)  # 文本文件列表\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fit_on_text` 函数的作用是对输入的文本计算一些关键统计量，并对里面的元素进行索引。\n",
    "1. 依次遍历文本列表变量元素，对于每个字符串元素，使用 `text_to_word_sequence` 函数进行拆分，并统一为小写字符。\n",
    "2. 计算单词出现的总频率和在不同文件中出现的频率，并对单词表排序。\n",
    "3. 计算总的单词量，并对每一个单词建立一个总的索引和一个在不同文件中的索引。\n",
    "\n",
    "完成上述工作后，就可以对整个列表中的元素进行拆分了。\n",
    "\n",
    "```python\n",
    "word_sequences = tokenizer.texts_to_sequences(alltext)\n",
    "```\n",
    "\n",
    "转换为矩阵：\n",
    "- `Tokenizer.texts_to_matrix` \n",
    "- `Tokenizer.sequences_to_matrix`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 序列数据预处理\n",
    "**跳跃语法**（Skip Gram） 模型：是一种单词表述（Word Representation）模型，它把每个单词映射到一个 $M$ 维的空间，即 `Word2Vec`。\n",
    "\n",
    "在 Keras 的预处理模块中有一个 `skipgrams` 函数，将词向量索引标号，按照两种可选方式转换为一系列两两元素的组合 $(w1, w2)$ 和标注 $z$。如果$w2$ 跟 $w1$ 是紧挨着的，则标注 $z$ 为 $1$，为正样本；如果 $w2$ 是从不相邻的其他元素中随机抽取的，则标注为 $0$，即负样本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [序列预处理](http://keras-cn.readthedocs.io/en/latest/preprocessing/sequence/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[3, 2],\n",
       "  [2, 1],\n",
       "  [1, 3],\n",
       "  [3, 1],\n",
       "  [1, 2],\n",
       "  [2, 3],\n",
       "  [2, 1],\n",
       "  [3, 2],\n",
       "  [1, 1],\n",
       "  [3, 1],\n",
       "  [2, 1],\n",
       "  [1, 2]],\n",
       " [0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = skipgrams([1, 2, 3], 3)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([3, 2], 0)\n",
      "([2, 1], 1)\n",
      "([1, 3], 1)\n",
      "([3, 1], 0)\n",
      "([1, 2], 0)\n",
      "([2, 3], 1)\n",
      "([2, 1], 0)\n",
      "([3, 2], 1)\n",
      "([1, 1], 0)\n",
      "([3, 1], 1)\n",
      "([2, 1], 0)\n",
      "([1, 2], 1)\n"
     ]
    }
   ],
   "source": [
    "res = list(zip(z[0], z[1]))\n",
    "for s in res:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 图片数据的输入\n",
    "`keras.preprocessing.image.ImageDataGenerator` 类生成一个数据生成器（Generator）对象，依照循环批量产生对应于图像信息的多维矩阵。\n",
    "- [图片预处理](http://keras-cn.readthedocs.io/en/latest/preprocessing/image/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Keras 中的模型\n",
    "依据拓扑结构分为：\n",
    "- 序列模型（Sequential 类）：属于通用模型的一个子类。\n",
    "- 通用模型（Model 类）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 序列模型\n",
    "这种模型各层之间是依次顺序的线性关系，在第 $k$ 层和 $k+1$ 层之间可以加上各种元素来构造神经网络。这些元素可以通过一个列表来制定，然后作为参数传递给序列模型来生成相应的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [Dense(32, input_shape= (784,)), \n",
    "         Activation('relu'),\n",
    "         Dense(10),\n",
    "         Activation('softmax')]\n",
    "model = Sequential(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了直接在一个列表中指定所有元素外，也可以像下面这样逐层添加："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape= (784,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 通用模型\n",
    "通用模型可以用来设计非常复杂、任意拓扑结构的神经网络，例如有向无环网络、共享层网络等。类似于序列模型，通用模型通过函数化的应用接口来定义模型。\n",
    "\n",
    "在定义的时候，从输入的多维矩阵开始，然后定义各层及其要素，最后定义输出层。将输入层与输出层作为参数纳入通用模型中就可以定义一个模型对象，并进行编译和拟合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
