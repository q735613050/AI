{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [MXBoard — 助力 MXNet 数据可视化](https://zh.mxnet.io/blog/mxboard)\n",
    "\n",
    "GitHub:[Logging MXNet Data for Visualization in TensorBoard](https://github.com/awslabs/mxboard#installation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T08:16:30.636891Z",
     "start_time": "2018-05-03T08:16:26.896237Z"
    }
   },
   "source": [
    "# MXBoard 快速上手指南\n",
    "\n",
    "所有的记录 API 都定义在一个叫 `SummaryWriter` 的类当中，这个类含有诸如记录的文件地址、写文件的频率、写文件的队列大小等等信息，用户可以根据需求设置。当需要把当前数据记录成 TensorBoard 中某种数据类型时，用户只要调用相应的 API 即可。\n",
    "\n",
    "画一个正态分布标准差逐渐缩小的数据分布图。首先定义一个写记录的对象如下，它会把数据写入到当前文件夹下的名为 `logs` 的文件夹。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T11:23:08.390972Z",
     "start_time": "2018-05-03T11:22:54.327811Z"
    }
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxboard import SummaryWriter\n",
    "\n",
    "sw = SummaryWriter(logdir='D:/logs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着在每个循环里，用 MXNet 的随机正态分布算子创建一个 NDArray，把这个 NDArray 传给写数据的 API `add_histogram`，指定画分布图时 `bin` 的数量和当前的循环数。最后，和 Python 里常用的文件写入器一样，记得关闭这个 `SummaryWriter`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T11:25:26.321523Z",
     "start_time": "2018-05-03T11:25:26.232291Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    # create a normal distribution with fixed mean and decreasing std\n",
    "    data = mx.nd.random.normal(loc=0, scale=10.0/(i+1), shape=(10, 3, 8, 8))\n",
    "    sw.add_histogram(tag='norml_dist', values=data, bins=200, global_step=i)\n",
    "sw.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了看到效果图，打开命令行窗口，进入到当前文件夹，键入如下命令以打开 TensorBoard：\n",
    "```sh\n",
    "tensorboard --logdir=./logs --host=127.0.0.1 --port=8888\n",
    "```\n",
    "\n",
    "接着在浏览器地址栏输入 `127.0.0.1:8888`，点击 `HISTOGRAM`，就可以看到效果图了。\n",
    "\n",
    "# 实战 MXBoard\n",
    "- 监督模型训练\n",
    "- 理解卷积神经网络的工作原理\n",
    "\n",
    "## [训练 MNIST 模型](https://github.com/awslabs/mxboard/tree/master/examples/mnist)\n",
    "借用 Gluon 里训练 MNIST 模型的 Python 程序，用 MXBoard 记录下交叉熵、训练和测试精度、参数的梯度数据分布，可以实时反映出模型训练的进度。\n",
    "\n",
    "- 首先定义一个 `SummaryWriter` 的对象："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T12:05:30.914187Z",
     "start_time": "2018-05-03T12:05:30.894172Z"
    }
   },
   "outputs": [],
   "source": [
    "sw = SummaryWriter(logdir='D:/logs', flush_secs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里加了 `flush_secs=5` 是为了每五秒就写一次记录到文件，以便在浏览器中及时看到结果。接着在每个 `mini-batch` 循环结束时记录下交叉熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T08:45:09.196504Z",
     "start_time": "2018-05-03T08:45:09.190501Z"
    }
   },
   "source": [
    "```py\n",
    "sw.add_scalar(tag='cross_entropy', value=L.mean().asscalar(), global_step=global_step)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T12:35:21.017588Z",
     "start_time": "2018-05-03T12:35:20.519856Z"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn\n",
    "from mxnet import gluon, autograd\n",
    "import numpy as np\n",
    "\n",
    "net = nn.HybridSequential()\n",
    "with net.name_scope():\n",
    "    net.add(nn.Dense(128, activation='relu'))\n",
    "    net.add(nn.Dense(64, activation='relu'))\n",
    "    net.add(nn.Dense(10))\n",
    "\n",
    "\n",
    "# data\n",
    "\n",
    "def transformer(data, label):\n",
    "    data = data.reshape((-1,)).astype(np.float32) / 255\n",
    "    return data, label\n",
    "\n",
    "batch_size = 64\n",
    "train_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.MNIST('E:/Data/MXNet/mnist',\n",
    "                            train=True, transform=transformer),\n",
    "    batch_size=batch_size, shuffle=True, last_batch='discard')\n",
    "\n",
    "val_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.MNIST('E:/Data/MXNet/mnist',\n",
    "                            train=False, transform=transformer),\n",
    "    batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "def test(ctx):\n",
    "    metric = mx.metric.Accuracy()\n",
    "    for data, label in val_data:\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        metric.update([label], [output])\n",
    "    return metric.get()\n",
    "\n",
    "\n",
    "ctx = mx.gpu(0)\n",
    "net.initialize(ctx=ctx, init=mx.init.MSRAPrelu())\n",
    "net.hybridize()\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "#trainer = gluon.Trainer(net.collect_params(), 'adadelta', {'rho': 0.9999})\n",
    "trainer = gluon.Trainer(net.collect_params(), 'rmsprop', {'learning_rate': 0.01, 'gamma1': 0.9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T12:35:21.069596Z",
     "start_time": "2018-05-03T12:35:21.065598Z"
    }
   },
   "outputs": [],
   "source": [
    "params = net.collect_params()\n",
    "param_names = list(params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T12:54:35.943814Z",
     "start_time": "2018-05-03T12:54:24.700340Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 100] Training: accuracy=0.961634\n",
      "[Epoch 0 Batch 200] Training: accuracy=0.961210\n",
      "[Epoch 0 Batch 300] Training: accuracy=0.960029\n",
      "[Epoch 0 Batch 400] Training: accuracy=0.960061\n",
      "[Epoch 0 Batch 500] Training: accuracy=0.960454\n",
      "[Epoch 0 Batch 600] Training: accuracy=0.960301\n",
      "[Epoch 0 Batch 700] Training: accuracy=0.961216\n",
      "[Epoch 0 Batch 800] Training: accuracy=0.961513\n",
      "[Epoch 0 Batch 900] Training: accuracy=0.961362\n"
     ]
    }
   ],
   "source": [
    "# define a summary writer that logs data and flushes to the file every 5 seconds\n",
    "sw = SummaryWriter(logdir='D:/logs', flush_secs=5)\n",
    "global_step = 0\n",
    "ctx = mx.gpu(0)\n",
    "metric = mx.metric.Accuracy()\n",
    "epoch = 0\n",
    "metric.reset()\n",
    "for i, (data, label) in enumerate(train_data):\n",
    "    # Copy data to ctx if necessary\n",
    "    data = data.as_in_context(ctx)\n",
    "    label = label.as_in_context(ctx)\n",
    "    # Start recording computation graph with record() section.\n",
    "    # Recorded graphs can then be differentiated with backward.\n",
    "    with autograd.record():\n",
    "        output = net(data)\n",
    "        L = loss(output, label)\n",
    "        sw.add_scalar('cross_entropy', {'mnist' : L.mean().asscalar()}, global_step=global_step)\n",
    "        global_step += 1\n",
    "    L.backward()\n",
    "    # take a gradient step with batch_size equal to data.shape[0]\n",
    "    trainer.step(data.shape[0])\n",
    "    # update metric at last.\n",
    "    metric.update([label], [output])\n",
    "    if i % 100 == 0 and i > 0:\n",
    "        # how many batches to wait before logging training status\n",
    "        name, train_acc = metric.get()\n",
    "        print(('[Epoch %d Batch %d] Training: %s=%f' %(epoch, i, name, train_acc)))\n",
    "    if i == 0:\n",
    "        sw.add_image('mnist_first_minibatch', data.reshape((batch_size, 1, 28, 28)), epoch)\n",
    "if epoch == 0:\n",
    "    sw.add_graph(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T14:42:49.618061Z",
     "start_time": "2018-05-03T14:42:48.115319Z"
    }
   },
   "outputs": [],
   "source": [
    "grads = [i.grad() for i in net.collect_params().values()]\n",
    "assert len(grads) == len(param_names)\n",
    "# logging the gradients of parameters for checking convergence\n",
    "for i, name in enumerate(param_names):\n",
    "    sw.add_histogram(tag=name, values=grads[i], global_step=epoch, bins=1000)\n",
    "name, acc = metric.get()\n",
    "# logging training accuracy\n",
    "sw.add_scalar(tag='train_acc', value=acc, global_step=epoch)\n",
    "\n",
    "name, val_acc = test(ctx)\n",
    "# logging the validation accuracy\n",
    "sw.add_scalar(tag='valid_acc', value=val_acc, global_step=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在每个 `epoch` 结束时记录下参数的梯度为 `HISTOGRAM`，记录下训练和测试精度为 `SCALAR`，\n",
    "```py\n",
    "grads = [i.grad() for i in net.collect_params().values()]\n",
    "assert len(grads) == len(param_names)\n",
    "# logging the gradients of parameters for checking convergence\n",
    "for i, name in enumerate(param_names):\n",
    "    sw.add_histogram(tag=name, values=grads[i], global_step=epoch, bins=1000)\n",
    "\n",
    "name, acc = metric.get()\n",
    "# logging training accuracy\n",
    "sw.add_scalar(tag='train_acc', value=acc, global_step=epoch)\n",
    "\n",
    "name, val_acc = test(ctx)\n",
    "# logging the validation accuracy\n",
    "sw.add_scalar(tag='valid_acc', value=val_acc, global_step=epoch)\n",
    "```\n",
    "\n",
    "然后运行 Python 程序，并运行 TensorBoard，就可以在浏览器中看到以下效果了。小伙伴们可以尝试着用 MXBoard 监督训练更复杂神经网络。更多本实例的代码和解说请点击[这里](https://github.com/reminisce/mxboard-demo#monitoring-training-mnist-model)。\n",
    "\n",
    "## 可视化卷积层的 filters 和 feature maps\n",
    "\n",
    "将卷积层的 `filters` 和 `feature maps` 当成图片可视化有两个意义：\n",
    "- 特征平滑规律的 `filters` 是模型训练良好的标志之一，未收敛或过拟合模型的卷积层 `filters` 会出现很多 noise。\n",
    "- 观察 `filters` 和 `feature maps` 的图片，特别是第一层卷积的图片可以总结出该层所关注的图片特征，这有助于我们理解卷积神经网络的工作原理。\n",
    "\n",
    "这里将 [MXNet Model Zoo](https://mxnet.incubator.apache.org/model_zoo/index.html) 中三个 CNN 模型，`Inception-BN`，`Resnet-152`，和 `VGG16` 的 `filters` 当成图像输出到 `TensorBoard`，并将这三组 `filters` 作用于一张黑天鹅的图片（来自验证数据集 `val_256_q90.rec`）上观察 `feature maps`。\n",
    "\n",
    "可以看出三个模型的 filters 都表现出良好的光滑性和规律性，彩色 filters 负责提取原始图片前景和背景的局部特征，灰白图片负责提取图片中物体的轮廓特征。复现代码和解释请点击[这里](https://github.com/reminisce/mxboard-demo#visualizing-filters-of-convnets)。\n",
    "\n",
    "## 可视化图片的 `embedding`\n",
    "\n",
    "最后这个例子比较有趣。Embedding 在自然语言处理中是一个常用的概念，它是真实世界中物体在高维向量空间中的表示。我们也可以借用此概念到卷积神经网络中。卷积神经网络最后一个全连接层的输出可以看成是一个 `batch_size` 行、`num_labels` 列的矩阵，每一行作为一个 `num_labels` 维的向量就是对应输入图片的 embedding。本质上这个 embedding 就是卷积神经网络对图片的编码，softmax 层通过此编码来判断图片所属类别。当理解了图片 embedding 的概念后，我们就可以把一个图片集的所有 embedding 通过没有 softmax 层的卷积神经网络求出来，调用 MXBoard 的 `add_embedding` API，从而来观察他们在二维或者三维空间中的聚合效应，即同类别图片应该聚合在一起。\n",
    "这里我们从上一个例子里的验证数据集中随机选取了$2304$张图片，用 Resnet-152 模型算出了它们的 embeddings，用 MXBoard 写入事件文件，并由 TensorBoard 读取。\n",
    "\n",
    "这里$2304$张图片的 embeddings 默认由 PCA 算法压缩到了三维空间，不过图片聚合效应似乎不是那么明显，这是因为 PCA 算法不能保持原始物体之间的空间关系。因此，我们选用 TensorBoard 界面上提供的 `t-SNE` 算法，重新对 embeddings 进行降维操作，这是个动态的过程。\n",
    "\n",
    "随着 `t-SNE` 算法的收敛，可以很明显地看到图片集在三维空间中被分成了几类。\n",
    "\n",
    "最后我们来验证一下图片分类是否正确。在 TensorBoard GUI的右上角输入”`dog`”，所有打了”`dog`”标记的图片将被高亮。拖动并放大至高亮图片处，可以看到很多狗的图片，这表明预训练的 Resnet-152 模型是准确的。\n",
    "全部代码和具体说明请点[这里](https://github.com/reminisce/mxboard-demo#visualizing-convnet-codes-as-embeddings)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "183px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
